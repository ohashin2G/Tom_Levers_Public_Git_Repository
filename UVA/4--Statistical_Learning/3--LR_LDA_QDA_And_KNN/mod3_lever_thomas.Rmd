---
title: "DS-6030 Homework Module 3"
author: "Tom Lever"
date: 06/08/2023
output:
  pdf_document: default
  html_document: default
urlcolor: blue
---

<!--- Below are global settings for knitr. You can override any of them by adding the changes to individual chunks --->
```{r global_options, include = FALSE}
# This chunk is called global_options. Due to `include = FALSE`, when the document is rendered, the chunk will be executed, but the results and code will not be included in the rendered document
knitr::opts_chunk$set(
    error = TRUE, # Keep compiling upon error
    collapse = FALSE, # code and corresponding output appear in knit file in separate blocks
    echo = TRUE, # echo code by default
    comment = "#", # change comment character
    #fig.width = 5.5, # set figure width
    fig.align = "center", # set figure position
    #out.width = "49%", # set width of displayed images
    warning = TRUE, # do not show R warnings
    message = TRUE # do not show R messages
)
```

**DS 6030 | Spring 2023 | University of Virginia **

<!--
5.  We now examine the differences between LDA and QDA.

    (a) If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?
    
        If the Bayes decision boundary is linear, we expect Quadratic Discriminant Analysis to perform better on the training set. According to [https://online.stat.psu.edu/stat508/lesson/9/9.2/9.2.8](https://online.stat.psu.edu/stat508/lesson/9/9.2/9.2.8), "QDA, because it allows for more flexibility for the covariance matrix, tends to fit the data better than LDA". We expect Linear Discriminant Analysis to perform better on the test set as the Bayes decision boundary is linear and QDA might overfit the data / follow errors too closely / yield a small training Mean Squared Error but a large test MSE / work too hard to find patterns in the training data and pick up some patterns that are just caused by random chance rather than by true properties of the function relating predictors and response.

    (b) If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?
    
        If the Bayes decision boundary is non-linear, we expect QDA to perform better on the training set and test set "because it allows for more flexibility for the covariance matrix".

    (c) In general, as the sample size $n$ increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?
    
        According to [https://cseweb.ucsd.edu/classes/sp12/cse151-a/lecture11-final.pdf](https://cseweb.ucsd.edu/classes/sp12/cse151-a/lecture11-final.pdf), "Variance depends on the training set size. It decreases with more training data, and increases with more complicated classifiers". As the sample size $n$ increases, we expect the test prediction accuracy of QDA relative to LDA to improve as QDA is a more complicated, flexible model than LDA with less bias and more variance than LDA and the variance of QDA decreases as sample size increases.

    (d) True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.

        False. As above, we expect Linear Discriminant Analysis to perform better on the test set when the Bayes decision boundary is linear as QDA might overfit the data.


13. This question should be answered using the `Weekly` data set, which is part of the `ISLR2` package.

    This data is similar in nature to the `Smarket` data from this chapterâ€™s lab, except that it contains $1,089$ weekly returns for 21 years, from the beginning of 1990 to the end of 2010.

    (a) Produce some numerical and graphical summaries of the `Weekly` data. Do there appear to be any patterns?
    
        ```{r}
        library(ISLR2)
        head(x = Weekly, n = 3)
        ```
        
        The columns of `Weekly` are
        - $Year$: The year that the observation was recorded
        - $Lag1$: Percentage returns $1$ week previous
        - $Lag2$: Percentage returns $2$ weeks previous
        - $Lag3$: Percentage returns $3$ weeks previous
        - $Lag4$: Percentage returns $4$ weeks previous
        - $Lag5$: Percentage returns $5$ weeks previous
        - $Volume$: Volume of stock market movement / volume of stock market activity / volume of shares traded / average number of daily shares traded in billions this week
        - $Today$: Percentage return in the S&P500 this week
        - $Direction$: Factor with levels `Down` and `Up` indicating whether the market had a positive or negative return on a given week
        
        ```{r}
        summary(Weekly)
        ```
        
        The years that an observation was recorded vary from $1990$ to $2010$.
        Minimum, first-quartile, median, mean, third-quartile, and maximum lags are similar across $Lag1$, $Lag2$, $Lag3$, $Lag4$, and $Lag5$ and $Today$.
        The market had a negative return for $484$ weeks. The market had a positive return for $605$ weeks.
        If we predicted that the market had a positive return for every one of the $1,089$ weeks, we would be correct $605 / 1,089 = 55.6$ percent of the time.
        
        ```{r}
        pairs(Weekly)
        plot(
            x = Weekly$Year,
            y = Weekly$Volume,
            xlab = "Year",
            ylab = "Volume",
            main = "Volume vs. Year"
        )
        ```
        
        $Volume$ seems to grow exponentially with $Year$.
        
        ```{r}
        library(TomLeversRPackage)
        index_of_Direction <- get_index_of_column_of_data_frame(Weekly, "Direction")
        data_frame_without_Direction <- Weekly[, -index_of_Direction]
        correlation_matrix <- cor(data_frame_without_Direction)
        analyze_correlation_matrix(correlation_matrix)
        ```
        
        $Volume$ has a high positive correlation with $Year$. All other pairs of variables have correlations that are negligible.
 
    (b) Use the full data set to perform a logistic regression with `Direction` as the response and the five lag variables plus `Volume` as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?
    
        ```{r}
        LR_Model_With_Formula_Direction_Vs_Lag1_Lag2_Lag3_Lag4_Lag5_and_Volume <- glm(
            formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
            data = Weekly,
            family = binomial
        )
        summary(LR_Model_With_Formula_Direction_Vs_Lag1_Lag2_Lag3_Lag4_Lag5_and_Volume)
        calculate_critical_value_zc(
            significance_level = 0.05,
            hypothesis_test_is_two_tailed = TRUE
        )
        ```
        
        A critical value $z_{\alpha/2 = 0.05/2} = 1.960$. The summary for the above logistic regression model provides test statistics for predictors. In parallel, the summary provides probabilities where each probability $p$ is the probability that the magnitude $|z|$ of a random test statistic is greater than the magnitude $|z_0|$ of the appropriate test statistic. Because the magnitude of the test statistic for $Lag2$ is greater than the critical value, and the probability for this predictor is less than the significance level $\alpha = 0.05$, we reject the null hypothesis that $Lag2$ is insignificant in predicting the response in the context of the model and can be removed from the model. For $Lag2$ we have sufficient evidence to support the alternate hypothesis that the predictor is significant in predicting the response in the context of the model and cannot be removed from the model.

    (c) Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.
    
        ```{r}
        generate_summary_of_performance(
            type_of_model = "LR",
            formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
            training_data = Weekly,
            test_data = Weekly
        )
        ```
        
        The overall fraction of correct predictions is $611 / 1,089$. The confusion matrix is telling us that there are $48$ false negatives and $430$ false positives. A false negative is an instance of our logistic regression predicting that the market had a negative return on a week when the market had a positive return on that week. A false positive is an instance of our logistic regression predicting that the market had a positive return on a week when the market had a negative return on that week. The training error rate is $43.9$ percent. For weeks when the market had a positive return, the model is correct $92.1$ percent of the time / has a sensitivity, recall, hit rate, and True Positive Rate $TPR = 0.921$. For weeks when the market had a negative return, the model is correct $11.2$ percent of the time / has a specificity, selectivity, and True Negative Rate $TNR = 0.112$.

    (d) Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).
    
        ```{r}
        condition <- (Weekly$Year >= 1990) & (Weekly$Year <= 2008)
        Weekly_from_1990_to_2008_inclusive <- Weekly[condition, ]
        condition <- (Weekly$Year > 2008) & (Weekly$Year <= 2010)
        Weekly_from_2009_to_2010_inclusive <- Weekly[condition, ]
        generate_summary_of_performance(
            type_of_model = "LR",
            formula = Direction ~ Lag2,
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive
        )
        ```
        
        The overall fraction of correct predictions is $65 / 104$. The test error rate is $37.5$ percent. For weeks when the market had a positive return, the model is correct $92.1$ percent of the time / has a sensitivity, recall, hit rate, and True Positive Rate $TPR = \frac{TP}{P} = \frac{TP}{TP + FN} = \frac{56}{56 + 5} = 0.918$. For weeks when the market had a negative return, the model is correct $11.2$ percent of the time / has a specificity, selectivity, and True Negative Rate $TNR = \frac{TN}{N} = \frac{TN}{TN + FP} = \frac{34}{34 + 9} = 0.209$.

    (e) Repeat (d) using LDA.
    
        ```{r}
        library(MASS)
        LDA_Model_With_Formula_Direction_Vs_Lag2 <- lda(
            formula = Direction ~ Lag2,
            data = Weekly_from_1990_to_2008_inclusive
        )
        LDA_Model_With_Formula_Direction_Vs_Lag2
        ```
        
        According to [https://www.andreaperlato.com/mlpost/linear-discriminant-analysis/](https://www.andreaperlato.com/mlpost/linear-discriminant-analysis/), "Linear Discriminant Analysis was originally developed by R.A. Fisher to classify subjects into one of... two clearly defined groups. It was later expanded to classify subjects into more than two groups. [LDA] helps to find linear combination[s] of original variables that provide[s] the best possible separation between the groups."
        
        According to [http://strata.uga.edu/8370/lecturenotes/discriminantFunctionAnalysis.html](http://strata.uga.edu/8370/lecturenotes/discriminantFunctionAnalysis.html), LDA for two groups "seeks a linear function that will maximum the differences among the groups... LDA will find an equation that maximizes the separation of the two groups using the variables measured for the cases in those two groups. If there are three variables in the data set $(x, y, z)$, the discriminant function has the following linear form:
        $$DF = a \left(x - \bar{x}\right) + b \left(y - \bar{y}\right) + c \left(z - \bar{z}\right)$$
        where $a$, $b$, and $c$ are the coefficients (slopes) of the discriminant function. Each sample or case will therefore have a single value called its score.
        
        Linear Discriminant Analysis "produces a number of discriminant functions (similar to principal components, and sometimes called axes) equal to the number of groups to be distinguished minus one."
        
        For our LDA model, we have two groups. One group contains observations where each observation corresponds to a week when the market had a positive return. One group contains observations where each observation corresponds to a week when the market had a negative return. We have one predictor: $Lag2$.
        
        "Coefficients of linear discriminants" "reports the coefficients of the discriminant function ($a$, $b$, and $c$). Because there are two groups, there are $2 - 1 = 1$ discriminant functions. Our one discriminant function
        
        $$LD1 = \beta_{Lag2} \left(Lag2 - \bar{Lag2}\right) = 0.441 \left(Lag2 - 0.128\right)$$
        
        The group means are "average values of each of the variables for each of your groups." The mean value for $Lag2$ for our observations between $1990$ and $2008$ and for the group of weeks when the market had a positive return is $0.260$. The mean value for $Lag2$ for our observations between $1990$ and $2008$ and for the group of weeks when the market had a negative return is $-0.036$. For a week when the market had a positive return, the return two weeks previously was likely positive. For a week when the market had a negative return, the return two weeks previously was likely negative.
        
        "The prior probabilities of the groups... reflect... the proportion of each group within the dataset. In other words, if you had no measurements and the number of measured samples represented the actual abundances of the groups, the prior probabilities would describe the probability that any unknown sample would belong to each of the groups." The market had a positive return on a given week $55.2$ percent of the time.
        
        "Distribution Of Discriminant Function Values Corresponding To Observations With Direction 'Up'" and "Distribution of Discriminant Function Values Corresponding To Observations With Direction 'Down'" are plotted below. There is poor "separation of the groups" along discriminant function 1". 
        
        ```{r}
        # The height of each version of the below histograms produced by
        # `plot(LDA_Model_With_Formula_Direction_Vs_Lag2)` is about 2.25 inches.
        prediction <- predict(
            object = LDA_Model_With_Formula_Direction_Vs_Lag2,
            newdata = Weekly_from_1990_to_2008_inclusive
        )
        vector_of_discriminant_function_values <- prediction$x
        training_observations_have_direction_Up <-
            Weekly_from_1990_to_2008_inclusive$Direction == "Up"
        training_observations_have_direction_Down <-
            Weekly_from_1990_to_2008_inclusive$Direction == "Down"
        indices_of_observations_with_direction_Up <-
            which(training_observations_have_direction_Up)
        indices_of_observations_with_direction_Down <-
            which(training_observations_have_direction_Down)
        vector_of_discriminant_function_values_corresponding_to_direction_Up <-
            vector_of_discriminant_function_values[
                indices_of_observations_with_direction_Up
            ]
        vector_of_discriminant_function_values_corresponding_to_direction_Down <-
            vector_of_discriminant_function_values[
                indices_of_observations_with_direction_Down
            ]
        hist(
            x = vector_of_discriminant_function_values_corresponding_to_direction_Up,
            xlim = c(-8, 6),
            breaks = 20,
            xlab = paste(
                "Discriminant Function Values Corresponding To\n",
                "Observations With Direction 'Up'",
                sep = ""
            ),
            ylab = "Frequency",
            main = paste(
                "Distribution Of Discriminant Function Values Corresponding To\n",
                "Observations With Direction 'Up'",
                sep = ""
            )
        )
        hist(
            x = vector_of_discriminant_function_values_corresponding_to_direction_Down,
            xlim = c(-8, 6),
            breaks = 20,
            xlab = paste(
                "Discriminant Function Values Corresponding To\n",
                "Observations With Direction 'Down'",
                sep = ""
            ),
            ylab = "Frequency",
            main = paste(
                "Distribution Of Discriminant Function Values Corresponding To\n",
                "Observations With Direction 'Down'",
                sep = ""
            )
        )
        #ldahist
        #    data = vector_of_discriminant_function_values,
        #    g = Weekly_from_1990_to_2008_inclusive$Direction
        #)
        #plot(LDA_Model_With_Formula_Direction_Vs_Lag2)
        ```
        
        ```{r}
        generate_summary_of_performance(
            type_of_model = "LDA",
            formula = Direction ~ Lag2,
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive
        )
        ```
        
        The overall fraction of correct predictions is $65 / 104$. Our Linear Discriminant Analysis model has fraction of correct predictions equal to that of our Logistic Regression model. The test error rate is $37.5$ percent. For weeks when the market had a positive return, the model is correct $91.8$ percent of the time / has a sensitivity, recall, hit rate, and True Positive Rate $TPR = \frac{TP}{P} = 0.918$. For weeks when the market had a negative return, the model is correct $20.9$ percent of the time / has a specificity, selectivity, and True Negative Rate $TNR = 0.209$.

    (f) Repeat (d) using QDA.
    
        ```{r}
        generate_summary_of_performance(
            type_of_model = "QDA",
            formula = Direction ~ Lag2,
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive
        )
        ```
        
        The overall fraction of correct predictions is $61 / 104$. Our Quadratic Discriminant Analysis model predicts all observations as having direction "Up". Our Logistic Regression and Linear Discriminant Analysis models have fractions of correct predictions greater than that of our Quadratic Discriminant Analysis model. The test error rate is $41.3$ percent. For weeks when the market had a positive return, the model is correct $100$ percent of the time / has a sensitivity, recall, hit rate, and True Positive Rate $TPR = 1$. For weeks when the market had a negative return, the model is correct $0$ percent of the time / has a specificity, selectivity, and True Negative Rate $TNR = 0$.

    (g) Repeat (d) using KNN with $K = 1$.
    
        ```{r}
        set.seed(1)
        generate_summary_of_performance(
            type_of_model = "KNN",
            formula = Direction ~ Lag2,
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive,
            K = 1
        )
        ```
        
        The overall fraction of correct predictions is $52 / 104$. Our Logistic Regression, Linear Discriminant Analysis, and Quadratic Discriminant Analysis models have fractions of correct predictions greater than that of our K Nearest Neighbors model. The test error rate is $50$ percent. For weeks when the market had a positive return, the model is correct $50.8$ percent of the time / has a sensitivity, recall, hit rate, and True Positive Rate $TPR = 0.508$. For weeks when the market had a negative return, the model is correct $48.8$ percent of the time / has a specificity, selectivity, and True Negative Rate $TNR = 48.8$.

    (h) Repeat (d) using naive Bayes. (skip this exercise)

    (i) Which of these methods appears to provide the best results on this data?
    
        Our Logistic Regression and Linear Discriminant Analysis models have the greatest overall fraction of correct predictions of $65 / 104$. Our Quadratic Discriminant Analysis model has an overall fraction of correct predictions of $61 / 104$. Our K Nearest Neighbors model has an overall fraction of correct predictions of $52 / 104$.

    (j) Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for $K$ in the KNN classifier.
    
        According to [https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html](https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html), "The DHARMa package uses a simulation-based approach to create readily interpretable scaled (quantile) residuals for fitted (generalized) linear mixed models. The resulting residuals are standardized to values between $0$ and $1$ and can be interpreted as intuitively as residuals from linear regression."
        
        "The interpretation of conventional residuals for generalized linear (mixed) models and other hierarchical statistical models is often problematic... [M]isspecifications in GL(M)Ms cannot reliably be diagnosed with standard residual plots, and thus GLMMs are often not as thoroughly checked as they should."
        
        "One reason why GL(M)Ms' residuals are harder to interpret is that the expected distribution of the data (aka predictive distribution) changes with the fitted values. Reweighting with the expected dispersion, as done in Pearson residuals, or using deviance residuals, helps to some extent, but it does not lead to visually homogeneous residuals, even if the model is correctly specified. As a result, standard residual plots, when interpreted in the same way as for linear models, such as non-normality, [and] heteroscedasticity, even if the model is correctly specified. Questions on the R mailing lists and forums show that practitioners are regularly confused about whether such patterns in GL(M)M residuals are a problem or not."
        
        "[E]ven experienced statistical analysts currently have few options to diagnose misspecification problems in GL(M)Ms. In my experience, the current standard practice is to eyeball the residual plots for major misspecifications, potentially have a look at the random effect distribution, and then run a test for overdispersion, which is usually positive, after which the model is modified twoard an overdispersed / zero-inflated distribution. This approach, however, has a number of drawbacks, notably:"
        
        - "Overdispersion is often the result of missing predictors or a misspecified model structure. Standard residual plots make it difficult to identify these problems by examining residual correlations or patterns of residuals against predictors..."
        
        - "Moreover, if residuals are checked, they are usually checked conditional on the fitted random effect estimates. Thus, standard checks only check the final level of the random structure in a GL(M)M. One can perform extra checks on the random effects, but it is somewhat unsatisfactory that there is no check on the entire model structure."
        
        "DHARMa aims at solving these problems by creating readily interpretable residuals for generalized linear (mixed) models that are standardized to values between $0$ and $1$, and that can be interpreted as intuitively as residuals for the linear model. This is achieved by a simulation-based approach, similar to the Bayesian $p$-value or the parametric bootstrap, that transforms the residuals to a standardized scale. The basic steps are:
        
        - "Simulate new response data from the fitted model for each observation."
        
        - "For each observation, calculate the empirical cumulative density function for the simulated [response data], which describes the possible values [of the response] (and their probabilit[ies]) at the predictor combination of the observed value, assuming the fitted model is correct.
        
        - "The residual is defined as the value of the empirical [cumulative] density function at the value of the observed data, so a residual of $0$ means that all simulated values are larger than the observed value, and a residual of $0.5$ means half of the simulated values are larger than the observed value."
        
        "The key advantage of this definition is that the so-defined residuals always have the same, known distribution, independent of the model that is fit, if the model is correctly specified. To see this, note that, if the observed data was created from the same data-generating process that we simulate from, all values of the cumulative distribution should appear with equal probability. That means we expect the distribution of the residuals to be flat, regardless of the model structure (Poisson, binomial, random effects and so on)."
    
        ```{r}
        library(DHARMa)
        LR_Model_With_Formula_Direction_Vs_Lag1 <- glm(
            formula = Direction ~ Lag1,
            data = Weekly_from_1990_to_2008_inclusive,
            family = binomial
        )
        DHARMa <- simulateResiduals(
            fittedModel = LR_Model_With_Formula_Direction_Vs_Lag1,
            plot = TRUE
        )
        simulated_residuals <- residuals(DHARMa)
        acf(simulated_residuals)
        ```
    
        ```{r}
        LR_Model_With_Formula_Direction_Vs_Lag2 <- glm(
            formula = Direction ~ Lag2,
            data = Weekly_from_1990_to_2008_inclusive,
            family = binomial
        )
        DHARMa <- simulateResiduals(
            fittedModel = LR_Model_With_Formula_Direction_Vs_Lag2,
            plot = TRUE
        )
        simulated_residuals <- residuals(DHARMa)
        acf(simulated_residuals)
        ```
        
        Assumptions for the following logistic model are met. We do not need to transform the predictors of our logistic regression models.
        $$Direction = ln\left[\frac{P(Y = 1 \ | \ X)}{1 - P(Y = 1 \ | \ X)}\right] = \beta_0 + \beta_1 \ X = 0.203 + 0.058 \ X; X \in \{Lag1, \ Lag2\}$$

        - The relationship between $L$ and $X$ is linear; the residuals are evenly scattered across $e = 0.5$.
        
        - The mean residual is $0.5$; the residuals are evenly scattered across $e = 0.5$.
        
        - The variance of the residuals is constant; the residuals are evenly spread for different predicted values.
        
        - The residuals are uncorrelated; AutoCorrelation Function values are approximately insignificant. The logistic regression model is robust to this assumption.
        
        - The residuals are normally distributed.

        ```{r}
        intercept_only_model <- glm(
            formula = Direction ~ 1,
            data = Weekly_from_1990_to_2008_inclusive,
            family = binomial
        )
        full_model <- glm(
            formula = Direction ~ Year + Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
            data = Weekly_from_1990_to_2008_inclusive,
            family = binomial
        )
        step(
            intercept_only_model,
            scope = list(lower = intercept_only_model, upper = full_model),
            direction = "both"
        )
        ```
        
        Logistic Regression model with formulas $Direction \sim Lag1$ and $Direction \sim Lag2$ have the same Akaike Information Criterion. An LR model with formula $Direction \sim Lag1 + Lag2$ has a smaller Akaike Information Criterion. Logistic regression models with further predictors have higher Akaike Information Criteria. We choose as a working model an LR model with formula $Direction \sim Lag1 + Lag2$. Assumptions for this model are met.
        
        ```{r}
        LR_Model_With_Formula_Direction_Vs_Lag1_And_Lag2 <- glm(
            formula = Direction ~ Lag1 + Lag2,
            data = Weekly_from_1990_to_2008_inclusive,
            family = binomial
        )
        DHARMa <- simulateResiduals(
            fittedModel = LR_Model_With_Formula_Direction_Vs_Lag1_And_Lag2,
            plot = TRUE
        )
        simulated_residuals <- residuals(DHARMa)
        acf(simulated_residuals)
        ```
        
        Evaluating the fraction of correct predictions for various Logistic Regression models involving $Lag1$ and $Lag2$, the fraction of correct predictions is highest for an LR model with formula $Direction \sim Lag2$.
        
        ```{r}
        library(TomLeversRPackage)
        summary_of_performance <- generate_summary_of_performance(
            type_of_model = "LR",
            formula = Direction ~ Lag1,
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive
        )
        summary_of_performance$decimal_of_correct_predictions
        summary_of_performance <- generate_summary_of_performance(
            type_of_model = "LR",
            formula = Direction ~ Lag2,
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive
        )
        summary_of_performance$decimal_of_correct_predictions
        summary_of_performance <- generate_summary_of_performance(
            type_of_model = "LR",
            formula = Direction ~ Lag1 + Lag2,
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive
        )
        summary_of_performance$decimal_of_correct_predictions
        # Lag1:Lag2 denotes an interation term.
        # Lag1*Lag2 denotes Lag1 + Lag2 + Lag1:Lag2
        summary_of_performance <- generate_summary_of_performance(
            type_of_model = "LR",
            formula = Direction ~ Lag1:Lag2,
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive
        )
        summary_of_performance$decimal_of_correct_predictions
        ```
        
        Since our Logistic Regression and LDA models with formulas $Direction \sim Lag2$ have the same fraction of correct predictions, we assume our LDA models will behave similarly to our Logistic Regression models. We compare LDA models with formulas $Direction \sim Lag1$, $Direction \sim Lag2$, $Direction \sim Lag1 + Lag2$, and $Direction \sim Lag1 \ Lag2$. Our Logistic Regression and LDA models with formulas $Direction \sim Lag2$ have the highest fraction of correct predictions of $0.625$.
        
        ```{r}
        summary_of_performance <- generate_summary_of_performance(
            type_of_model = "LDA",
            formula = Direction ~ Lag1,
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive
        )
        summary_of_performance$decimal_of_correct_predictions
        summary_of_performance <- generate_summary_of_performance(
            type_of_model = "LDA",
            formula = Direction ~ Lag2,
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive
        )
        summary_of_performance$decimal_of_correct_predictions
        summary_of_performance <- generate_summary_of_performance(
            type_of_model = "LDA",
            formula = Direction ~ Lag1 + Lag2,
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive
        )
        summary_of_performance$decimal_of_correct_predictions
        # Lag1:Lag2 denotes an interation term.
        # Lag1*Lag2 denotes Lag1 + Lag2 + Lag1:Lag2
        summary_of_performance <- generate_summary_of_performance(
            type_of_model = "LDA",
            formula = Direction ~ Lag1:Lag2,
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive
        )
        summary_of_performance$decimal_of_correct_predictions
        ```
        
        We experiment with Quadratic Discriminant Analysis models. A QDA model with formula $Direction \sim Lag2$ has a fraction of correct predictions of $0.587$ that is less than the fraction of correct predictions of an LDA model with formula $Direction \sim Lag2$ of $0.625$. We attempt to compensate for the increase in variance with a common transform of predictor $Lag2$. Applying the absolute-value and log or square-root functions to predictor $Lag2$ has no effect. Applying the square function to predictor $Lag2$ results in a drop in fraction of correct predictions to $0.567$.
        
        ```{r}
        summary_of_performance <- generate_summary_of_performance(
            type_of_model = "QDA",
            formula = Direction ~ Lag1,
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive
        )
        summary_of_performance$decimal_of_correct_predictions
        summary_of_performance <- generate_summary_of_performance(
            type_of_model = "QDA",
            formula = Direction ~ Lag2,
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive
        )
        summary_of_performance$decimal_of_correct_predictions
        summary_of_performance <- generate_summary_of_performance(
            type_of_model = "QDA",
            formula = Direction ~ Lag1 + Lag2,
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive
        )
        summary_of_performance$decimal_of_correct_predictions
        summary_of_performance <- generate_summary_of_performance(
            type_of_model = "QDA",
            formula = Direction ~ Lag1:Lag2,
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive
        )
        summary_of_performance$decimal_of_correct_predictions
        summary_of_performance <- generate_summary_of_performance(
            type_of_model = "QDA",
            formula = Direction ~ I(Lag2^2),
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive
        )
        summary_of_performance$decimal_of_correct_predictions
        ```
        
        We experiment with $K$ Nearest Neighbors classifiers with different predictors and values of $K$. For a KNN classifier with predictor $Lag2$, increasing $K$ and decreasing variance by a factor of $187$ results in an increase in fraction of correct predictions from $0.5$ to $0.635$. For a KNN classifier with predictors $Lag1$ and $Lag2$ and $K = 375$, the fraction of correct predictions was $0.644$.

        ```{r}
        set.seed(1)
        summary_of_performance <- generate_summary_of_performance(
            type_of_model = "KNN",
            formula = Direction ~ Lag2,
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive,
            K = 187
        )
        summary_of_performance$decimal_of_correct_predictions
        summary_of_performance <- generate_summary_of_performance(
            type_of_model = "KNN",
            formula = Direction ~ Lag1 + Lag2,
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive,
            K = 350
        )
        summary_of_performance$decimal_of_correct_predictions
        summary_of_performance <- generate_summary_of_performance(
            type_of_model = "KNN",
            formula = Direction ~ Lag1 : Lag2,
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive,
            K = 350
        )
        summary_of_performance$decimal_of_correct_predictions
        summary_of_performance <- generate_summary_of_performance(
            type_of_model = "KNN",
            formula = Direction ~ sqrt(abs(Lag2)),
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive,
            K = 350
        )
        summary_of_performance$decimal_of_correct_predictions
        ```
        
        Through our experiments we discovered that our KNN model with predictors $Lag1$ and $Lag2$ and $K = 350$ had highest fraction of correct predictions of $0.644$ based on above confusion matrix. LR and LDA models with formula $Direction \sim Lag2$ had fractions of correct predictions of $0.625$ based on confusion matrices presented in parts (d) and (e). The summary of performance for our $KNN$ classifier is below.
        
        ```{r}
        generate_summary_of_performance(
            type_of_model = "KNN",
            formula = Direction ~ Lag1 + Lag2,
            training_data = Weekly_from_1990_to_2008_inclusive,
            test_data = Weekly_from_2009_to_2010_inclusive,
            K = 350
        )
        ```

-->

14. In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the `Auto` data set.

    (a) Create a binary variable, `mpg01`, that contains a 1 if `mpg` contains a value above its median, and a 0 if `mpg` contains a value below its median. You can compute the median using the `median()` function. Note you may find it helpful to use the `data.frame()` function to create a single data set containing both `mpg01` and the other `Auto` variables.
    
        ```{r}
        head(Auto, n = 3)
        median_fuel_efficiency <- median(Auto$mpg)
        number_of_observations <- nrow(Auto)
        mpg01 <- rep(0, number_of_observations)
        condition <- Auto$mpg > median_fuel_efficiency
        mpg01[condition] <- 1
        data_frame <- data.frame(Auto, mpg01)
        ```

    (b) Explore the data graphically in order to investigate the association between `mpg01` and the other features. Which of the other features seem most likely to be useful in predicting `mpg01`? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.
    
        Response $mpg01$ has a high positive correlation with $mpg$, high negative correlations with $cylinders$, $displacement$, and $weight$, a moderate positive correlation with $origin$, a moderate negative correlation with $horsepower$, and low positive correlations with $acceleration$ and $year$. $mpg$, $cylinders$, $displacement$, and $weight$ seem most likely to be useful in predicting $mpg01$.
        
        According to scatterplots, boxplots, and a barplot, obviously, all low fuel efficiencies correspond to $mpg01 = 0$ and all high fuel efficiencies correspond to $mpg01 = 1$. All non-outlying high fuel efficiencies correspond to the first quarter of number of cylinders and to four cylinders. High displacements correspond to low fuel efficiency; all non-outlying high fuel efficiencies correspond to the first quarter of displacements. High horsepowers correspond to low fuel efficiency; all non-outlying high fuel efficiencies correspond to the first half of horsepowers. High weights correspond to low fuel efficiency; all non-outlying high fuel efficiencies correspond to the first half of weights. Low accelerations correspond to low fuel efficiency; the first quartile of accelerations corresponding to high fuel efficiencies is greater than the median acceleration corresponding to low fuel efficiencies. The first quartile of years corresponding to high fuel efficiencies is higher than the median year corresponding to low fuel efficiencies; fuel efficiency increases with year. A supermajority of American automobiles have low fuel efficiencies. A supermajority of European automobiles have high fuel efficiencies. A supermajority of Japanese automobiles have high fuel efficiencies. A supermajority of automobiles with low fuel efficiency are American. Automobiles with high fuel efficiency are evenly distributed across American, European, and Japanese origin. There are about 3 times as many American automobiles as European automobiles and about 3 times as many American automobiles as Japanese automobiles.
        
        ```{r}
        library(TomLeversRPackage)
        index_of_column_name <- get_index_of_column_of_data_frame(data_frame, "name")
        data_frame_of_columns_except_name <- data_frame[, -index_of_column_name]
        correlation_matrix <- cor(data_frame_of_columns_except_name)
        analyze_correlation_matrix(correlation_matrix)
        plot(x = data_frame$mpg, y = data_frame$mpg01)
        boxplot(data_frame$mpg ~ data_frame$mpg01, data = data_frame)
        plot(x = data_frame$cylinders, y = data_frame$mpg01)
        boxplot(data_frame$cylinders ~ data_frame$mpg01, data = data_frame)
        plot(x = data_frame$displacement, y = data_frame$mpg01)
        boxplot(data_frame$displacement ~ data_frame$mpg01, data = data_frame)
        plot(x = data_frame$horsepower, y = data_frame$mpg01)
        boxplot(data_frame$horsepower ~ data_frame$mpg01, data = data_frame)
        plot(x = data_frame$weight, y = data_frame$mpg01)
        boxplot(data_frame$weight ~ data_frame$mpg01, data = data_frame)
        plot(x = data_frame$acceleration, y = data_frame$mpg01)
        boxplot(data_frame$acceleration ~ data_frame$mpg01, data = data_frame)
        plot(x = data_frame$year, y = data_frame$mpg01)
        boxplot(data_frame$year ~ data_frame$mpg01, data = data_frame)
        plot(x = data_frame$origin, y = data_frame$mpg01)
        boxplot(data_frame$origin ~ data_frame$mpg01, data = data_frame)
        table_of_origins_and_indicators_of_fuel_efficiency <- table(
            data_frame$origin,
            data_frame$mpg01
        )
        table_of_proportions_split_by_rows <- prop.table(
            x = table_of_origins_and_indicators_of_fuel_efficiency,
            margin = 1
        )
        table_of_percents <- table_of_proportions_split_by_rows * 100
        table_of_rounded_percents <- round(table_of_percents, 2)
        table_of_rounded_percents
        table_of_proportions_split_by_rows <- prop.table(
            x = table_of_origins_and_indicators_of_fuel_efficiency,
            margin = 2
        )
        table_of_percents <- table_of_proportions_split_by_rows * 100
        table_of_rounded_percents <- round(table_of_percents, 2)
        table_of_rounded_percents
        vector_of_English_indicators_of_origin <-
            rep("American", number_of_observations)
        condition <- data_frame$origin == 2
        vector_of_English_indicators_of_origin[condition] <- "European"
        condition <- data_frame$origin == 3
        vector_of_English_indicators_of_origin[condition] <- "Japanese"
        vector_of_English_indicators_of_fuel_efficiency <-
            rep("low", number_of_observations)
        condition <- data_frame$mpg01 == 1
        vector_of_English_indicators_of_fuel_efficiency[condition] <- "high"
        library(ggplot2)
        ggplot(
            data = data_frame,
            mapping = aes(
                x = vector_of_English_indicators_of_origin,
                fill = vector_of_English_indicators_of_fuel_efficiency
            )
        ) +
            geom_bar(position = "stack") +
            labs(
                x = "origin",
                y = "count",
                title = "Fuel Efficiency By Origin"
            ) +
             theme(
                 plot.title = element_text(hjust = 0.5)
             ) +
             guides(
                 fill = guide_legend(
                     title = "Fuel Efficiency"
                 )
             )
        ```

    (c) Split the data into a training set and a test set.

    (d) Perform LDA on the training data in order to predict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained?

    (e) Perform QDA on the training data in order to predict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained?

    (f) Perform logistic regression on the training data in order to predict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained?

    (g) Perform naive Bayes on the training data in order to predict `mpg01` using the variables that seemed most associated with `mpg01` in (b). What is the test error of the model obtained? (skip this exercise)

    (h) Perform KNN on the training data, with several values of $K$, in order to predict `mpg01`. Use only the variables that seemed most associated with `mpg01` in (b). What test errors do you obtain? Which value of $K$ seems to perform the best on this data set?