---
title: "DS-6030 Homework Module 1"
author: "Tom Lever"
date: 05/25/2023
output:
  pdf_document: default
  html_document: default
urlcolor: blue
---

<!--- Below are global settings for knitr. You can override any of them by adding the changes to individual chunks --->
```{r global_options, include = FALSE}
knitr::opts_chunk$set(
    error = TRUE, # Keep compiling upon error
    collapse = FALSE, # code and corresponding output appear in knit file in separate blocks
    echo = TRUE, # echo code by default
    comment = "#", # change comment character
    #fig.width = 5.5, # set figure width
    fig.align = "center", # set figure position
    #out.width = "49%", # set width of displayed images
    warning = FALSE, # do not show R warnings
    message = FALSE # do not show R messages
)
```

<!--- Change font size for headers --->
<style>
    h1.title {
        font-size: 28px;
    }
    h1 {
        font-size: 22px;
    }
    h2 {
        font-size: 18px;
    }
    h3 {
        font-size: 14px;
    }
</style>

**DS 6030 | Spring 2022 | University of Virginia **

1.  Flexible vs Inflexible Methods

    For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.

    (a) The sample size $n$ is extremely large, and the number of predictors $p$ is small.

        We would expect generally the performance of a flexible statistical learning method to be better than the performance of an inflexible method as a flexible statistical learning method would fit the extremely large set of data more closely.

    (b) The number of predictors $p$ is extremely large, and the number of observations $n$ is small.

        We would expect generally the performance of a flexible statistical learning method to be worse than the performance of an inflexible method as a flexible statistical learning method would overfit the small set of data.
    
    (c) The relationship between the predictors and response is highly non-linear.
    
        We would expect generally the performance of a flexible statistical method to be better than the performance of an inflexible method as a flexible statistical method has more degrees of freedom.

    (d) The variance of the error terms, i.e. $\sigma^2 = Var(\epsilon)$, is extremely high.
    
        We would expect generally the performance of a flexible statistical method to be worse than the performance of an inflexible method as a flexible statistical method would fit the variance, error, and noise of the set of data.

2.  Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide $n$ and $p$.

    (a) We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.

        This scenario is an inference regression problem as we are interested in understanding which predictors affect continuous response CEO salary. The number of samples and top firms $n = 500$. The number of predictors, including profit, number of employees, and industry, $p = 3$.

    (b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.

        This scenario is a prediction classification problem as we wish to know whether a new product will be a success or a failure. The number of samples and similar products that were previously launched $n = 20$. The number of predictors, including price charged for the product, marketing budget, competition price, and ten other variables, $p = 13$.

    (c) We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.

        This scenario is a prediction regression problem as we are interested in predicting the percent change in the US Dollar / Euro exchange rate in relation to the weekly changes in the world stock markets in 2012. The number of samples and weekly records $n = 52$. The number of predictors, including the percent change in the US market, the percent change in the British market, and the percent change in the German market, $p = 3$.

6.  Describe the differences between a parametric and a non-parametric statistical learning approach. 

    What are the advantages of a parametric approach to regression or classification (as opposed to a non-parametric approach)? What are its disadvantages?

    A parametric approach to regression and classification has the advantages of involving a explicit, understandable formula with generally a relatively small number of parameters with values that are trained based on a modest set of data. A parametric approach generally is simple, may be used for prediction and inference, is inflexible, and may have low variance among predicted values given different training data sets. A parametric approach may be too simple, inflexible, and have a high bias. Variance

    $$v = E\left(\left\{\hat{f}(x)-E\left[\hat{f}(x)\right]\right\}^2\right)$$

    where $\hat{f}(x)$ is the predicted value of a statistical learning model at $x$ and $E\left[\hat{f}(x)\right]$ is the expected predicted value of a statistical learning model at $x$ given different training data sets. Bias

    $$b = \left\{E\left[\hat{f}(x)\right] - y(x)\right\}^2$$
    where $y(x)$ is the ground-truth value at $x$ corresponding to the predicted value.

    A non-parametric approach to regression and classification has the advantage of modeling generally nonlinear relationships using a relatively large number of parameters with values that are trained based on a large set of data. A non-parametric approach is flexible and has a low bias. A non-parametric approach may be used for prediction. A non-parametric approach generally is a complex black box that may not be used for inference. A non-parametric approach may have high variance among predicted values given different training data sets and may overfit the set of data.

8.  This exercise relates to the `College` data set, which can be found in the file `College.csv` on the book website.

    It contains a number of variables for $777$ different universities and colleges in the US. The variables are

    - `Private` : Public/private indicator
    - `Apps` : Number of applications received
    - `Accept` : Number of applicants accepted
    - `Enroll` : Number of new students enrolled
    - `Top10perc` : New students from top 10 % of high school class 
    - `Top25perc` : New students from top 25 % of high school class 
    - `F.Undergrad` : Number of full-time undergraduates
    - `P.Undergrad` : Number of part-time undergraduates
    - `Outstate` : Out-of-state tuition
    - `Room.Board` : Room and board costs
    - `Books` : Estimated book costs
    - `Personal` : Estimated personal spending
    - `PhD` : Percent of faculty with Ph.D.’s
    - `Terminal` : Percent of faculty with terminal degree • S.F.Ratio : Student/faculty ratio
    - `perc.alumni` : Percent of alumni who donate
    - `Expend` : Instructional expenditure per student
    - `Grad.Rate` : Graduation rate

    Before reading the data into R, it can be viewed in Excel or a text editor.

    (a) Use the `read.csv()` function to read the data into R. Call the loaded data college. Make sure that you have the directory set to the correct location for the data.

        ```{r}
        college = read.csv("Colleges.csv")
        ```

    (b) Look at the data using the `View()` function. You should notice that the first column is just the name of each university. We don’t really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands:

        ```{r}
        rownames(college) <- college[, 1]
        ```
        
        ```{r, eval = FALSE}
        View(college)
        ```

        Now you should see that the first data column is `Indicator_Of_Whether_College_Is_Private`. Note that another column labeled `row names` now appears before the `Indicator_Of_Whether_College_Is_Private` column. However, this is not a data column but rather the name that R is giving to each row.
  
        ```{r}
        college <- college[, -1]
        ```
        
        ```{r, eval = FALSE}
        View(college)
        ```
    
        Now you should see that the first data column with name `Name_Of_College` has been removed from the data frame. Note that another column labeled `row names` now appears. However, this is not a data column but rather a column of the names that R is giving to each row.
  
    (c) See below.

        i.  Use the `summary()` function to produce a numerical summary of the variables in the data set.
        
            ```{r}
            summary(college)
            ```
        
        ii. Use the `pairs()` function to produce a scatterplot matrix of the first ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix `A` using `A[,1:10]`.
        
            ```{r}
            number_of_rows_in_college <- nrow(college)
            column_of_numerical_representations <- rep(0, number_of_rows_in_college)
            condition <- college$Indicator_Of_Whether_College_Is_Private == "Yes"
            column_of_numerical_representations[condition] <- 1
            data_frame_of_second_through_tenth_columns <- college[, 2:10]
            data_frame_representing_first_ten_columns_of_college <- data.frame(
                column_of_numerical_representations,
                data_frame_of_second_through_tenth_columns
            )
            pairs(
                data_frame_representing_first_ten_columns_of_college,
                main = "Scatterplot Matrix Of The First Ten Columns Of College",
                labels = seq(1, 10)
            )
            ```
        
        iii. Use the `plot()` function to produce side-by-side boxplots of `Outstate` versus `Private`.
        
             ```{r}
             factor_Indicator_Of_Whether_College_Is_Private <- as.factor(
                 college$Indicator_Of_Whether_College_Is_Private
             )
             plot(
                 x = factor_Indicator_Of_Whether_College_Is_Private,
                 y = college$Out_Of_State_Tuition,
                 main = "Distributions Of Out Of State Tuition\nBy Indicator Of Whether College Is Private",
                 xlab = "Indicator",
                 ylab = "Out Of State Tuition"
             )
             ```
        
        iv. Create a new qualitative variable, called `Elite`, by binning the `Top10perc` variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%.
  
            ```{r}
            number_of_rows_in_college <- nrow(college)
            Elite <- rep("No", number_of_rows_in_college)
            column_of_percents <- college$
                Percent_Of_New_Students_Who_Were_In_Top_10_Percent_Of_High_School_Class
            condition <- column_of_percents > 50
            Elite[condition] <- "Yes"
            Elite <- as.factor(Elite)
            college <- data.frame(college, Elite)
            ```
            
            Use the `summary()` function to see how many elite universities there are. Now use the `plot()` function to produce side-by-side boxplots of `Outstate` versus `Elite`.
            
            ```{r}
            summary(college$Elite)
            plot(
                x = Elite,
                y = college$Out_Of_State_Tuition,
                main = "Distributions Of Out Of State Tuition\nBy Indicator Of Whether College Is Elite",
                xlab = "Indicator",
                ylab = "Out Of State Tuition"
            )
            ```

        v.  Use the `hist()` function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command `par(mfrow = c(2, 2))` useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.
        
            ```{r}
            par(mfrow = c(2, 2))
            column_of_percents <- college$
                Percent_Of_New_Students_Who_Were_In_Top_25_Percent_Of_High_School_Class
            hist(
                x = column_of_percents,
                breaks = 6,
                main = "Percent Of New Students\nWho Were In Top 25 Percent\nOf High School Class",
                xlab = "Percent"
            )
            hist(
                x = college$Student_Faculty_Ratio,
                breaks = 7,
                main = "Student Faculty Ratio",
                xlab = "Ratio"
            )
            hist(
                x = college$Instructional_Expenditure_Per_Student,
                breaks = 8,
                main = "Instructional Expenditure\nPer Student",
                xlab = "Expenditure"
            )
            hist(
                x = college$Graduation_Rate,
                breaks = 9,
                main = "Graduation Rate",
                xlab = "Rate"
            )
            ```
        
        vi. Continue exploring the data, and provide a brief summary of what you discover.
        
            ```{r}
            column_Percent_Of_Faculty_With_PhDs <- college$Percent_Of_Faculty_With_PhDs
            summary(column_Percent_Of_Faculty_With_PhDs)
            college_has_percent_of_faculty_with_PhDs_equal_to_103 <-
                column_Percent_Of_Faculty_With_PhDs == 103
            index_of_college_with_percent_of_faculty_with_PhDs_equal_to_103 <- 
                which(college_has_percent_of_faculty_with_PhDs_equal_to_103)
            college[index_of_college_with_percent_of_faculty_with_PhDs_equal_to_103, ]
            ```
            
            In the `College` data set, Texas A&M University at Galveston has a percent of faculty with PhD's equal to $103$.
            This value could either be an error or a code for a missing value.
            
10. This exercise involves the Boston housing data set.

    (a) To begin, load in the `Boston` data set. The Boston data set is part of the ISLR2 library.
    
        ```{r}
        library(ISLR2)
        ```
        
        Now the data set is contained in the object Boston.
        
        ```{r}
        head(Boston)
        ```
        
        Read about the data set:
        
        ```{r, eval = FALSE}
        ?Boston
        ```
        
        How many rows are in this data set? How many columns? What do the rows and columns represent?
        
        ```{r}
        nrow(Boston)
        ncol(Boston)
        ```
        
        Rows are records containing "housing values in 506 suburbs of Boston" ([chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://cran.r-project.org/web/packages/ISLR2/ISLR2.pdf](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://cran.r-project.org/web/packages/ISLR2/ISLR2.pdf)). Columns represent features of suburbs of Boston and include:
        
        - `crim`: per capita crime rate by town
        - `zn`: proportion of residential land zoned for lots over $25,000$ square feet
        - `indus`: proportion of non-retail business acres per town
        - `chas`: Charles River dummy variable that is 1 if tract bounds river and 0 otherwise
        - `nox`: nitrogen oxides concentration in parts per 10 million
        - `rm`: average number of rooms per dwelling
        - `age`: proportion of owner-occupied units built prior to 1940
        - `dis`: weighted mean of distances to five Boston employment centers
        - `rad`: index of accessibility to radial highways
        - `tax`: full-value property-tax rate per 10,000 dollars
        - `ptratio`: pupil-teacher ratio by town
        - `lstat`: lower status of the population in percent
        - `medv`: median value of owner-occupied homes in thousands of dollars

    (b) Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.

        ```{r}
        plot(
            x = Boston$nox,
            y = Boston$crim,
            main = "Per Capita Crime Rate By Town vs. Nitrogen Oxides Concentration",
            xlab = "Nitrogen Oxides Concentration (Parts Per 10 Million)",
            ylab = "Per Capita Crime Rate By Town"
        )
        ```
        
        Generally, as nitrogen oxides concentration increases, per capita crime rate by town and its variance increase exponentially. Nitrogen oxides concentration may be a proxy for reduction in quiet green space allowing an individual to be themselves, industrialization, people being on top of each other, and reduction in health.
        
        ```{r}
        plot(
            x = Boston$age,
            y = Boston$crim,
            main = "Per Capita Crime Rate By Town vs.\nProportion Of Owner-Occupied Units Built Prior To 1940",
            xlab = "Proportion of Owner-Occupied Units Built Prior To 1940",
            ylab = "Per Capita Crime Rate By Town"
        )
        ```
        
        Generally, as the proportion of owner-occupied units built prior to 1940 increases, per capita crime rate by town and its variance increase exponentially. Proportion of owner-occupied units built prior to 1940 may be a proxy for proportion of substandard housing and urban decay.
        
        ```{r}
        plot(
            x = Boston$dis,
            y = Boston$crim,
            main = "Per Capita Crime Rate By Town vs.\nWeighted Mean Of Distances To Five Boston Employment Centers",
            xlab = "Weighted Mean Of Distances To Five Boston Employment Centers",
            ylab = "Per Capita Crime Rate By Town"
        )
        ```
        
        Generally, as the weighted mean of distances to five Boston employment centers increases, per capita crime rate by town and its variance decrease exponentially. Weighted mean of distances to five Boston employment centers may be a proxy for white flight or reduction in people being on top of each other.
       
        ```{r}
        plot(
            x = Boston$dis,
            y = Boston$nox,
            main = "Nitrogen Oxides Concentration vs.\nWeighted Mean Of Distances To Five Boston Employment Centers",
            xlab = "Weighted Mean Of Distances To Five Boston Employment Centers",
            ylab = "Nitrogen Oxides Concentration"
        )
        ```
        
        Generally, as the weighted mean of distances to five Boston employment centers increases, nitrogen oxides concentration decreases. Boston employment centers may be relatively industrialized and distant suburbs may be relatively green.
        
        ```{r}
        plot(
            x = Boston$lstat,
            y = Boston$ptratio,
            main = "Pupil Teacher Ratio vs. Proportion Of Population With Lower Status",
            xlab = "Proportion Of Population With Lower Status",
            ylab = "Pupil Teacher Ratio"
        )
        ```
        
        Generally, as proportion of population with lower status increases, pupil teacher ratio increases. Pupil teacher ratio may be a proxy for lack of individualized education.

    (c) Are any of the predictors associated with per capita crime rate? If so, explain the relationship.
    
        ```{r}
        correlation_matrix <- cor(Boston)
        library(TomLeversRPackage)
        analyze_correlation_matrix(correlation_matrix)
        ```
        
        Per capita crime rate is very highly positively associated with itself; highly correlated with no predictors; moderately positively correlated with index of accessibility of radial highways and full-value property-tax rate per 10,000 dollars; lowly positively correlated with proportion of non-retail business acres per town, nitrogen oxides concentration in parts per 10 million, proportion of owner-occupied units built prior to 1940, and lower status of the population in percent; and lowly negatively correlated with weighted mean of distances to five Boston employment centers and median value of owner-occupied homes in thousands of dollars.

    (d) Do any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.
    
        Many of the census tracts of Boston appear to have particularly high crime rates. These particularly high crime rates are represented below as outliers in "Distribution Of Crime Rates". According to "Distribution Of Tax Rates" and "Distribution Of Pupil Teacher Ratios", none of the census tracts of Boston appear to have particularly high tax rates or pupil teacher ratios. However, according to "Histogram Of Tax Rates", many of the census tracts of Boston appear to have particularly high tax rates and particularly high pupil teacher ratios. The minimum crime rate for a census tract of Boston is $0.006$; the maximum crime rate for a census tract of Boston is $89.0$. The minimum tax rate for a census tract of Boston is $187$; the maximum tax rate for a census tract of Boston is $711$. The minimum pupil teacher ratio for a census tract of Boston is $12.6$; the maximum pupil teacher ratio for a census tract of Boston is $22$.

        ```{r}
        boxplot(
            Boston$crim,
            main = "Distribution Of Crime Rates",
            ylab = "Crime Rate"
        )
        min(Boston$crim)
        max(Boston$crim)
        boxplot(
            Boston$tax,
            main = "Distribution Of Tax Rates",
            ylab = "Tax Rate"
        )
        min(Boston$tax)
        max(Boston$tax)
        hist(
            x = Boston$tax,
            main = "Histogram Of Tax Rates",
            xlab = "Tax Rate",
            ylab = "Frequency"
        )
        boxplot(
            Boston$ptratio,
            main = "Distribution Of Pupil Teacher Ratios",
            ylab = "Pupil Teacher Ratio"
        )
        min(Boston$ptratio)
        max(Boston$ptratio)
        hist(
            x = Boston$ptratio,
            main = "Histogram Of Pupil Teacher Ratios",
            xlab = "Pupil Teacher Ratio",
            ylab = "Frequency"
        )
        ```

    (e) How many of the census tracts in this data set bound the Charles river?
    
        ```{r}
        census_tracts_bound_Charles_River <- Boston$chas == 1
        vector_of_indices_of_census_tracts_that_bound_Charles_River <-
            which(census_tracts_bound_Charles_River)
        length(vector_of_indices_of_census_tracts_that_bound_Charles_River)
        ```

    (f) What is the median pupil-teacher ratio among the towns in this data set?
    
        ```{r}
        median(Boston$ptratio)
        ```

    (g) Which census tract of Boston has lowest median value of owner-occupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your findings.

        ```{r}
        index_of_lowest_median_home_value <- which.min(Boston$medv)
        index_of_lowest_median_home_value
        record_with_lowest_median_home_value <-
            Boston[index_of_lowest_median_home_value, ]
        record_with_lowest_median_home_value
        
        calculate_percentile_of_value_of_record_in_column <-
            function(record, name_of_column) {
            index <- get_column_index(record, name_of_column)
            value <- record[1, index]
            percentile <- calculate_percentile(Boston[, index], value)
            return(percentile)
        }
        
        calculate_percentile_of_value_of_record_in_column(
            record_with_lowest_median_home_value,
            "crim"
        )
        calculate_percentile_of_value_of_record_in_column(
            record_with_lowest_median_home_value,
            "zn"
        )
        calculate_percentile_of_value_of_record_in_column(
            record_with_lowest_median_home_value,
            "indus"
        )
        calculate_percentile_of_value_of_record_in_column(
            record_with_lowest_median_home_value,
            "chas"
        )
        calculate_percentile_of_value_of_record_in_column(
            record_with_lowest_median_home_value,
            "nox"
        )
        calculate_percentile_of_value_of_record_in_column(
            record_with_lowest_median_home_value,
            "rm"
        )
        calculate_percentile_of_value_of_record_in_column(
            record_with_lowest_median_home_value,
            "age"
        )
        calculate_percentile_of_value_of_record_in_column(
            record_with_lowest_median_home_value,
            "dis"
        )
        calculate_percentile_of_value_of_record_in_column(
            record_with_lowest_median_home_value,
            "rad"
        )
        calculate_percentile_of_value_of_record_in_column(
            record_with_lowest_median_home_value,
            "tax"
        )
        calculate_percentile_of_value_of_record_in_column(
            record_with_lowest_median_home_value,
            "ptratio"
        )
        calculate_percentile_of_value_of_record_in_column(
            record_with_lowest_median_home_value,
            "lstat"
        )
        calculate_percentile_of_value_of_record_in_column(
            record_with_lowest_median_home_value,
            "medv"
        )
        ```

    (h) In this data set, how many of the census tracts average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling.
    
        ```{r}
        census_tracts_average_more_than_seven_rooms_per_dwelling <- Boston$rm > 7
        census_tracts_average_more_than_eight_rooms_per_dwelling <- Boston$rm > 8
        vector_of_indices_of_tracts_averaging_more_than_seven_rooms_per_dwelling <- 
            which(census_tracts_average_more_than_seven_rooms_per_dwelling)
        vector_of_indices_of_tracts_averaging_more_than_eight_rooms_per_dwelling <-
            which(census_tracts_average_more_than_eight_rooms_per_dwelling)
        length(vector_of_indices_of_tracts_averaging_more_than_seven_rooms_per_dwelling)
        number_of_tracts_average_more_than_eight_rooms_per_dwelling <- length(
            vector_of_indices_of_tracts_averaging_more_than_eight_rooms_per_dwelling
        )
        number_of_tracts_average_more_than_eight_rooms_per_dwelling
        data_frame_of_tracts_averaging_more_than_eight_rooms_per_dwelling <-
            Boston[FALSE, ]
        for (i in 1:number_of_tracts_average_more_than_eight_rooms_per_dwelling) {
            index_of_tract_averaging_more_than_eight_rooms_per_dwelling <-
                vector_of_indices_of_tracts_averaging_more_than_eight_rooms_per_dwelling[
                    i
                ]
            number_of_rows <-
                nrow(data_frame_of_tracts_averaging_more_than_eight_rooms_per_dwelling)
            data_frame_of_tracts_averaging_more_than_eight_rooms_per_dwelling[
                number_of_rows + 1,
            ] <- Boston[i, ]
        }
        data_frame_of_tracts_averaging_more_than_eight_rooms_per_dwelling
        summary(Boston$crim)
        summary(data_frame_of_tracts_averaging_more_than_eight_rooms_per_dwelling$crim)
        summary(Boston$zn)
        summary(data_frame_of_tracts_averaging_more_than_eight_rooms_per_dwelling$zn)
        summary(Boston$indus)
        summary(data_frame_of_tracts_averaging_more_than_eight_rooms_per_dwelling$indus)
        summary(Boston$nox)
        summary(data_frame_of_tracts_averaging_more_than_eight_rooms_per_dwelling$nox)
        summary(Boston$age)
        summary(data_frame_of_tracts_averaging_more_than_eight_rooms_per_dwelling$age)
        summary(Boston$dis)
        summary(data_frame_of_tracts_averaging_more_than_eight_rooms_per_dwelling$dis)
        summary(Boston$rad)
        summary(data_frame_of_tracts_averaging_more_than_eight_rooms_per_dwelling$rad)
        summary(Boston$tax)
        summary(data_frame_of_tracts_averaging_more_than_eight_rooms_per_dwelling$tax)
        summary(Boston$ptratio)
        summary(data_frame_of_tracts_averaging_more_than_eight_rooms_per_dwelling$ptratio)
        summary(Boston$lstat)
        summary(data_frame_of_tracts_averaging_more_than_eight_rooms_per_dwelling$lstat)
        summary(Boston$medv)
        summary(data_frame_of_tracts_averaging_more_than_eight_rooms_per_dwelling$medv)
        ```
        
        The census tracts averaging more than eight rooms per dwelling generally have low crime rates, proportion of residential land zoned for lots over $25,000$ square feet, proportion of non-retail business acres per town, nitrogen oxides concentration in parts per 10 million, index of accessibility to radial highways, full-value property-tax rates per 10,000 dollars, and pupil teacher ratio by town. These census tracts generally have average proportion of owner-occupied units built prior to 1940 and percent of population with lower status. These census tracts have high average number of rooms per dwelling, weighted mean of distances to five Boston employment centers, and median value of owner-occupied homes in thousands of dollars. These census tracts do not bound the Charles River.