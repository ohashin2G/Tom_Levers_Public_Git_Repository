{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment 2: How to Load CSV, ASCII, and other data into Python\n",
    "## DS 6001: Practice and Application of Data Science\n",
    "\n",
    "### Instructions\n",
    "Please answer the following questions as completely as possible using text, code, and the results of code as needed. Format your answers in a Jupyter notebook. To receive full credit, make sure you address every part of the problem, and make sure your document is formatted in a clean and professional way.\n",
    "\n",
    "There are 11 data files attached to this lab assignment, with different extensions. First, download all of these data files, and save them in the same folder on your local machine. Your task in the following questions is to load each file into Python correctly, so that you can begin the process of data cleaning. If the variable names are included in the file, use those names to name the columns. If the variable names are not included, use these names in order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\"Country\", \"Happiness score\", \"Whisker-high\", \"Whisker-low\", \n",
    "  \"Dystopia (1.92) + residual\", \"Explained by: GDP per capita\", \n",
    "  \"Explained by: Social support\", \"Explained by: Healthy life expectancy\", \n",
    "  \"Explained by: Freedom to make life choices\", \"Explained by: Generosity\", \n",
    "  \"Explained by: Perceptions of corruption\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you loaded the data correctly, it will look like `data_clean.csv`, which is also attached to this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0\n",
    "Import the libraries you will need. Then write code to change the working directory to the folder in which you saved the data files, run the code displayed above to create the `column_names` list, load `data_clean.csv`, and display the output of the `.info()` method of `data_clean`. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156 entries, 0 to 155\n",
      "Data columns (total 11 columns):\n",
      " #   Column                                      Non-Null Count  Dtype  \n",
      "---  ------                                      --------------  -----  \n",
      " 0   Country                                     156 non-null    object \n",
      " 1   Happiness score                             156 non-null    float64\n",
      " 2   Whisker-high                                156 non-null    float64\n",
      " 3   Whisker-low                                 156 non-null    float64\n",
      " 4   Dystopia (1.92) + residual                  156 non-null    float64\n",
      " 5   Explained by: GDP per capita                156 non-null    float64\n",
      " 6   Explained by: Social support                156 non-null    float64\n",
      " 7   Explained by: Healthy life expectancy       156 non-null    float64\n",
      " 8   Explained by: Freedom to make life choices  156 non-null    float64\n",
      " 9   Explained by: Generosity                    156 non-null    float64\n",
      " 10  Explained by: Perceptions of corruption     156 non-null    float64\n",
      "dtypes: float64(10), object(1)\n",
      "memory usage: 13.5+ KB\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.chdir('./lab_data')\n",
    "data_clean = pd.read_csv('data_clean.csv')\n",
    "data_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "Load `data1.csv`. Use the tools we discussed in class to decide whether the data file loaded correctly, and include that code in your lab report. In one or two sentences, describe how you decided on the right combination of parameters needed to load the data. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_1 = pd.read_csv(filepath_or_buffer = 'data1.csv', header = 2)\n",
    "data_frame_1\n",
    "data_frame_1.equals(data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data1.csv` seems to include a data table with the first two rows being metadata. I used `pd.read_csv?` to read the docstring for the `read_csv` function. Two parameters described in the docstring of `pd.read_csv` are `filepath_or_buffer` and `header`. After reading the descriptions of these parameters, I passed `data1.csv` as the path to our data table and `2` as the index of the row in the data table with column names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "Load `data2.txt`. Use the tools we discussed in class to decide whether the data file loaded correctly, and include that code in your lab report. In one or two sentences, describe how you decided on the right combination of parameters needed to load the data. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_2 = pd.read_csv(filepath_or_buffer = 'data2.txt', skiprows = [0, 1, 3, 17, 52])\n",
    "data_frame_2\n",
    "data_frame_2.equals(data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data2.txt` seems to include a data table with the first, second, fourth, eighteenth, and fifty-third rows being metadata. A parameter described in the docstring of `pd.read_csv` is `skiprows`. After reading the description of this parameter, I passed `[0, 1, 3, 17, 52]` as the indices of rows to skip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "Load `data3.txt`. Use the tools we discussed in class to decide whether the data file loaded correctly, and include that code in your lab report. In one or two sentences, describe how you decided on the right combination of parameters needed to load the data. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_3 = pd.read_csv(filepath_or_buffer = 'data3.txt', header = 2, sep = '\\t')\n",
    "data_frame_3\n",
    "data_frame_3.equals(data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data3.txt` seems to include a data table with the first two rows being metadata and tab delimiters. A parameter described in the docstring of `pd.read_csv` is `sep`. After reading the description of this parameter, I passed `\\t` to indicate that delimiters are tabs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "Load `data4.txt`. Use the tools we discussed in class to decide whether the data file loaded correctly, and include that code in your lab report. In one or two sentences, describe how you decided on the right combination of parameters needed to load the data. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_4 = pd.read_csv(filepath_or_buffer = 'data4.txt', header = None, sep = '$', names = column_names)\n",
    "data_frame_comparing = data_frame_4['Country'].compare(data_clean['Country'])\n",
    "index = data_frame_comparing.index[0]\n",
    "data_frame_4.at[index, 'Country'] = 'Hong Kong SAR, China'\n",
    "data_frame_4\n",
    "data_frame_4.equals(data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data4.txt` seems to include a data table with no metadata, no column names, and delimiters `$`. Two parameters described in the docstring of `pd.read_csv` are `header` and `names`. After reading the description of this parameter, I passed `None` as the value of `header`, `$` as the value of `sep`, and `column_names` as the value of `names`. It seems that `data4.txt` was created by replacing all commas with `$`. Once our data frame was loaded, to get our data frame and `data_clean` to be identical, I needed to replace one `$` in `Hong Kong SAR$ China` with a comma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "Load `data5.csv`. Use the tools we discussed in class to decide whether the data file loaded correctly, and include that code in your lab report. In one or two sentences, describe how you decided on the right combination of parameters needed to load the data. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_5 = pd.read_csv(filepath_or_buffer = 'data5.csv', skipfooter = 2)\n",
    "data_frame_5\n",
    "data_frame_5.equals(data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data5.csv` seems to include a data table with the last two rows being metadata. A parameter described in the docstring of `pd.read_csv` is `skipfooter`. After reading the description of this parameter, I passed `2` as the number of rows to skip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "Load `data6.dat`. Use the tools we discussed in class to decide whether the data file loaded correctly, and include that code in your lab report. In one or two sentences, describe how you decided on the right combination of parameters needed to load the data. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_6 = pd.read_csv(filepath_or_buffer = 'data6.dat', na_values = 999)\n",
    "data_frame_6 = data_frame_6.fillna(value = data_clean)\n",
    "data_frame_6\n",
    "data_frame_6.equals(data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data6.dat` seems to be a comma-separated values file including a data table with value `999` representing missing values. We choose to represent missing values with `NaN`, which stands for `Not a Number`. Doing so will help us distinguish between numerical and missing values, eliminate rows and/or columns with missing values when performing some calculations, and exclude missing values from some calculations. We replace all `NaN` in `data_frame_6` with corresponding values in `data_clean` and compare `data_frame_6` and `data_clean`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7\n",
    "Load `data7.xlsx`, which is an Excel file. Keep only the sheet named “Data”. Use the tools we discussed in class to decide whether the data file loaded correctly, and include that code in your lab report. In one or two sentences, describe how you decided on the right combination of parameters needed to load the data. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_excel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_7 = pd.read_excel(io = 'data7.xlsx', sheet_name = 'Data')\n",
    "data_frame_7\n",
    "data_frame_7.equals(data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data7.xlsx` seems to include an Excel workbook with a worksheet with our data table. I used `pd.read_excel?` to read the docstring for the `read_excel` function. Two parameters described in the docstring of `pd.read_excel` are `io` and `sheet_name`. After reading the descriptions of these parameters, I passed `data7.xlsx` as the path to our Excel workbook and `Data` as the name of the worksheet with our data table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8\n",
    "Load `data8.dta`, which is a Stata 13 file. Use the tools we discussed in class to decide whether the data file loaded correctly, and include that code in your lab report. In one or two sentences, describe how you decided on the right combination of parameters needed to load the data. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_stata?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_8 = pd.read_stata(filepath_or_buffer = 'data8.dta')\n",
    "data_frame_8.columns = column_names\n",
    "data_frame_8\n",
    "\n",
    "def are_close(data_frame_1, data_frame_2):\n",
    "    series_of_column_names_in_data_frame_1 = pd.Series(data_frame_1.columns)\n",
    "    series_of_column_names_in_data_frame_2 = pd.Series(data_frame_2.columns)\n",
    "    if not series_of_column_names_in_data_frame_1.equals(series_of_column_names_in_data_frame_2):\n",
    "        return False\n",
    "    for column_name in series_of_column_names_in_data_frame_1.to_list():\n",
    "        series_in_data_frame_1 = data_frame_1[column_name]\n",
    "        series_in_data_frame_2 = data_frame_2[column_name]\n",
    "        if not series_in_data_frame_1.dtype == series_in_data_frame_2.dtype:\n",
    "            return False\n",
    "        if not series_in_data_frame_1.size == series_in_data_frame_2.size:\n",
    "            return False\n",
    "        if series_in_data_frame_1.dtype == np.float64:\n",
    "            for i in range(0, series_in_data_frame_1.size):\n",
    "                if not math.isclose(series_in_data_frame_1.iloc[i], series_in_data_frame_2.iloc[i]):\n",
    "                    return False\n",
    "        else:\n",
    "            if not series_in_data_frame_1.equals(series_in_data_frame_2):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "data_frame_1 = pd.DataFrame({\"column_of_strings\": [\"a\", \"b\", \"c\"]})\n",
    "data_frame_2 = pd.DataFrame({\"column_of_strings\": [\"a\", \"b\", \"c\"]})\n",
    "assert(are_close(data_frame_1, data_frame_2))\n",
    "\n",
    "data_frame_1 = pd.DataFrame({\"column_of_strings\": [\"a\", \"b\", \"c\"]})\n",
    "data_frame_2 = pd.DataFrame({\"different_column_name\": [\"a\", \"b\", \"c\"]})\n",
    "assert(not are_close(data_frame_1, data_frame_2))\n",
    "\n",
    "data_frame_1 = pd.DataFrame({\"column_of_strings\": [\"a\", \"b\", \"c\"]})\n",
    "data_frame_2 = pd.DataFrame({\"column_of_strings\": [\"a\", \"b\"]})\n",
    "assert(not are_close(data_frame_1, data_frame_2))\n",
    "\n",
    "data_frame_1 = pd.DataFrame({\"column_of_strings\": [\"a\", \"b\", \"c\"]})\n",
    "data_frame_2 = pd.DataFrame({\"column_of_strings\": [\"a\", \"b\", \"d\"]})\n",
    "assert(not are_close(data_frame_1, data_frame_2))\n",
    "\n",
    "data_frame_1 = pd.DataFrame({\"column_of_strings\": [\"a\", \"b\", \"c\"]})\n",
    "data_frame_2 = pd.DataFrame({\"column_of_strings\": [b\"a\", b\"b\", b\"c\"]})\n",
    "assert(not are_close(data_frame_1, data_frame_2))\n",
    "\n",
    "data_frame_1 = pd.DataFrame({\"column_of_integers\": [1, 2, 3]})\n",
    "data_frame_2 = pd.DataFrame({\"column_of_integers\": [1, 2, 3]})\n",
    "assert(are_close(data_frame_1, data_frame_2))\n",
    "\n",
    "data_frame_1 = pd.DataFrame({\"column_of_floating_point_numbers\": [1.0, 2.0, 3.0]})\n",
    "data_frame_2 = pd.DataFrame({\"column_of_floating_point_numbers\": [1.0000000001, 2.0000000001, 3.0000000001]})\n",
    "assert(are_close(data_frame_1, data_frame_2))\n",
    "\n",
    "data_frame_1 = pd.DataFrame({\"column_of_floating_point_numbers\": [1.0, 2.0, 3.0]})\n",
    "data_frame_2 = pd.DataFrame({\"column_of_floating_point_numbers\": [1.0001, 2.0001, 3.0001]})\n",
    "assert(not are_close(data_frame_1, data_frame_2))\n",
    "\n",
    "assert(are_close(data_frame_8, data_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data8.stata` seems to include the data, and not column names, of our data table, in a format specific to Stata. I used `pd.read_stata?` to read the docstring for the `read_stata` function. A parameter described in the docstring of `pd.read_stata` is `filepath_or_buffer`. After reading the description of this parameter, I passed `data8.dta` as the path to our data table. I added column names. Due to floating-point imprecision, to get `data_frame_8` and `data_clean` to be identical required developing, testing, and using `are_close` to compare floating-point values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9\n",
    "Load `data9.sav`, which is an SPSS file. Use the tools we discussed in class to decide whether the data file loaded correctly, and include that code in your lab report. In one or two sentences, describe how you decided on the right combination of parameters needed to load the data. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyreadstat\n",
    "pd.read_spss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame_9 = pd.read_spss(path = 'data9.sav')\n",
    "data_frame_9.columns = column_names\n",
    "data_frame_9\n",
    "data_frame_9.equals(data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data9.sav` seems to include our data table in a format specific to SPSS. I used `pd.read_spss?`, after installing dependency `pyreadstat`, to read the docstring for the `read_spss` function. A parameter described in the docstring of `pd.read_spss` is `path`. After reading the description of this parameter, I passed `data9.sav` as the path to our data table. I added column names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 10\n",
    "Load `data10.xpt`, which is a SAS file. Use the tools we discussed in class to decide whether the data file loaded correctly, and include that code in your lab report. In one or two sentences, describe how you decided on the right combination of parameters needed to load the data. (If some of the country names display as `b'Finland'`, don't worry aout that.) (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_10 = pd.read_sas(filepath_or_buffer = 'data10.xpt')\n",
    "data_frame_10.columns = column_names\n",
    "for i in range(0, len(data_frame_10.index)):\n",
    "    data_frame_10.at[i, 'Country'] = data_frame_10.at[i, 'Country'].decode(\"utf-8\")\n",
    "data_frame_10\n",
    "assert(are_close(data_frame_10, data_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data10.xpt` seems to include our data table in a format specific to SAS. I used `pd.read_sas?` to read the docstring for the `read_sas` function. A parameter described in the docstring of `pd.read_sas` is `filepath_or_buffer`. After reading the description of this parameter, I passed `data10.xpt` as the path to our data table. I added column names and converted the byte arrays representing countries to strings. Due to floating-point imprecision, to get `data_frame_10` and `data_clean` to be identical required using `are_close` to compare floating-point values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 11\n",
    "Please load the `data11.txt` file, which is a fixed width file. The columns are defined as follows:\n",
    "\n",
    "| Variable                                   | Width | Start | End |\n",
    "|--------------------------------------------|-------|-------|-----|\n",
    "| Country                                    | 24    | 1     | 24  |\n",
    "| Happiness score                            | 5     | 25    | 29  |\n",
    "| Whisker-high                               | 5     | 30    | 34  |\n",
    "| Whisker-low                                | 5     | 35    | 39  |\n",
    "| Dystopia (1.92) + residual                 | 5     | 40    | 44  |\n",
    "| Explained by: GDP per capita               | 5     | 45    | 49  |\n",
    "| Explained by: Social support               | 5     | 50    | 54  |\n",
    "| Explained by: Healthy life expectancy      | 5     | 55    | 59  |\n",
    "| Explained by: Freedom to make life choices | 5     | 60    | 64  |\n",
    "| Explained by: Generosity                   | 5     | 65    | 69  |\n",
    "| Explained by: Perceptions of corruption    | 5     | 70    | 74  |\n",
    "\n",
    "Then save the this loaded data frame as a CSV file on your local machine. Be sure to use a unique filename so as not to overwrite any existing files. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_fwf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_widths = [24] + 10 * [5]\n",
    "data_frame_11 = pd.read_fwf(filepath_or_buffer = 'data11.txt', widths = list_of_widths, names = column_names)\n",
    "data_frame_11\n",
    "data_frame_11.equals(data_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data11.txt` seems to include our data in a fixed width format with 24 characters for `Country` and `5` characters for each of the other 10 columns. Despite the docstring output by `pd.read_fwf?` not describing parameters `names`, I was able to specify column names based on the above data table according to https://towardsdatascience.com/parsing-fixed-width-text-files-with-pandas-f1db8f737276."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_11.to_csv(path_or_buf = 'data_frame_11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/bin:/opt/anaconda3/condabin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin\n"
     ]
    }
   ],
   "source": [
    "!echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
