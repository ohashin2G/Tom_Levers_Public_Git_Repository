---
title: "DS-6030 Homework Module 4"
author: "Tom Lever"
date: 06/13/2023
output:
  pdf_document: default
  html_document: default
urlcolor: blue
---

<!--- Below are global settings for knitr. You can override any of them by adding the changes to individual chunks --->
```{r global_options, include = FALSE}
# This chunk is called global_options. Due to `include = FALSE`, when the document is rendered, the chunk will be executed, but the results and code will not be included in the rendered document
knitr::opts_chunk$set(
    error = TRUE, # Keep compiling upon error
    collapse = FALSE, # code and corresponding output appear in knit file in separate blocks
    echo = TRUE, # echo code by default
    comment = "#", # change comment character
    #fig.width = 5.5, # set figure width
    fig.align = "center", # set figure position
    #out.width = "49%", # set width of displayed images
    warning = TRUE, # do not show R warnings
    message = TRUE # do not show R messages
)
```

**DS 6030 | Spring 2023 | University of Virginia **

3.  We now review $k$-fold cross-validation.

    (a) Explain how $k$-fold cross-validation is implemented.
    
        According to *An Introduction  to Statistical Learning* (Second Edition) by James et al., "$k$-fold cross validation involves randomly dividing a set of observations int $k$ groups, or folds, of approximately equal size. The first fold is treated as a validation set, and the method is fit on the remaining $k - 1$ folds."
        
        For models with continuous responses, we consider the error rate to be a linear combination of Mean Squared Errors. "The mean squared error, $MSE_1$, is then computed on the observations in the held-out fold. This procedure is repeated $k$ times; each time, a different group of observations is treated as a validation set. This process results in $k$ estimates of the test error, $MSE_1, MSE_2, ..., MSE_k$. The $k$-fold cross validation estimate is computed by averages these values,
        
        $$CV_k = \frac{1}{k} \sum_{i = 1}^k \left[MSE_i\right]$$
        
        Figure 5.5 illustrates the $k$-fold CV approach... In practice, one typically performs $k$-fold CV using $k = 5$ or $k = 10$.
    
        ![]("k-Fold_Cross_Validation.png")
        
        When we perform cross-validation, our goal might be to determine how well a given statistical learning procedure can be expected to perform on independent data; in this case, the actual estimate of the test MSE is of interest. But at other times we are interested only in the location of the minimum point in the estimated test MSE curve. This is because we might be performing cross-validation on a number of statistical learning methods, or on a single method using different levels of flexibility, in order to identify the method that results in the lowest test error... [D]espite the fact that they sometimes underestimate or [overestimate] the true test MSE, [most] of the CV curves come close to identifying the correct level of flexibility--that is, the flexibility level corresponding to the smallest test MSE."
        
        For models with qualitative responses, we consider the error rate to be a linear combination of numbers of misclassified observations. "For instance, in the classification setting, the LOOCV error rate takes the form
        
        $$Err_i = I\left(y_i \neq \hat{y}_i\right)$$
        $$CV_k = \frac{1}{k} \sum_{i = 1}^k \left[ Err_i \right]$$

    (b) What are the advantages and disadvantages of k-fold cross-validation relative to:
    
        i.  The validation set approach?
        
            The right-hand panel of Figure $5.4$ displays nine different $10$-fold CV estimates for the `Auto` data set, each resulting from a different random split of the observations into ten folds. As we can see from the figure, there is some variability in the CV estimates as a result of the variability in how the observations are divided into ten folds. But this variability is typically much lower than the variability in the test error estimates that result from the validation set approach (right-hand panel of Figure 5.2)."
        
            ![]("MSE_Vs_Degree_For_Cross-Validation_Approach.png")
            
            ![]("MSE_Vs_Degree_For_Validation-Set_Approach.png")
        
            According to [What are the advantages and disadvantages of using $k$-fold cross-validation for predictive analytics?](https://www.linkedin.com/advice/0/what-advantages-disadvantages-using-k-fold#:~:text=K%2Dfold%20cross%2Dvalidation%20has%20some%20drawbacks%20that%20need%20to,testing%20the%20model%20multiple%20times.), "K-fold cross-validation has some drawbacks that need to be considered. It increases the computational cost and time, as it requires training and testing the model multiple times. Additionally, it may not be suitable for some types of data, such as time-series or spatial data, due to the order or location of the observations. Furthermore, it may introduce bias if the data is not well distributed or balanced across the folds. Lastly, it may not account for the uncertainty or variability of the model's performance, as it only gives a point estimate."
        
        ii. LOOCV?
        
            LOOCV is a special case of $k$-fold CV in which $k$ is set to equal [number of observations] $n$... LOOCV requires fitting the statistical learning method $n$ times. This has the potential to be computationally expensive... But cross-validation is a very general approach that can be applied to almost any statistical learning method. Some statistical learning methods have computationally intensive fitting procedures, and so performing LOOCV may pose computational problems, especially in $n$ is extremely large. In contrast, performing $10$-fold CV requires fitting the learning procedure only ten times, which may be much more feasible.
            
            $k$-fold cross validation "often gives more accurate estimates of the test error rate than does LOOCV. This has to do with a bias-variance trade-off... from the perspective of bias reduction, it is clear that LOOCV is to be preferred to $k$-fold CV. However, we know that bias is not the only source for concern in an estimating procedure; we must also consider the procedure's variance. It turns out the LOOCV has higher variance than does $k$-fold CV with $k < n$... When we perform LOOCV, we are in effect averaging the outputs of $n$ fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other. In contrast, when we perform $k$-fold CV with $k < n$, we are averaging the outputs of $k$ fitted models that are somewhat less correlated with each other, since the overlap between the training sets in each model is smaller. Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from $k$-fold CV. To summarize, there is a bias-variance trade-off associated with the choice of $k$ in $k$-fold cross-validation. Typically, given these considerations, one performs $k$-fold cross-validation using $k = 5$ or $k = 10$, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance."

5.  In Chapter 4, we used logistic regression to predict the probability of `default` using `income` and `balance` on the `Default` data set.

    We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.

    (a) Fit a logistic regression model that uses `income` and `balance` to predict `default`.
    
        ```{r}
        library(ISLR2)
        LR_model <- glm(
            formula = default ~ income + balance,
            data = Default,
            family = binomial
        )
        LR_model
        ```

    (b) Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:

        i. Split the sample set into a training set and a validation set.

            ```{r}
            set.seed(1)
            library(TomLeversRPackage)
            split_data <- split_data_set_into_training_and_testing_data(
                data_frame = Default,
                proportion_of_training_data = 0.5
            )
            training_data <- split_data$training_data
            validation_data <- split_data$testing_data
            nrow(training_data)
            number_of_validation_observations <- nrow(validation_data)
            number_of_validation_observations
            ```

        ii. Fit a multiple logistic regression model using only the training observations.
        
            ```{r}
            LR_model <- glm(
                formula = default ~ income + balance,
                data = training_data,
                family = binomial
            )
            LR_model
            ```

        iii. Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the `default` category if the posterior probability is greater than 0.5.
                 
             ```{r}
             vector_of_predicted_probabilities <- predict(
                 object = LR_model,
                 newdata = validation_data,
                 type = "response"
             )
             vector_of_predicted_default_statuses <- rep(
                 x = "No",
                 times = number_of_validation_observations
             )
             condition <- vector_of_predicted_probabilities > 0.5
             vector_of_predicted_default_statuses[condition] = "Yes"
             vector_of_predicted_default_statuses[1:3]
             ```

        iv. Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.
        
            ```{r}
            mean(vector_of_predicted_default_statuses != training_data$default)
            ```
            
            The validation error rate is $4.8$ percent.

    (c) Repeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained.
    
        ```{r}
        vector_of_error_rates <- double(length = 0)
        for (i in 2:4) {
            set.seed(i)
            split_data <- split_data_set_into_training_and_testing_data(
                data_frame = Default,
                proportion_of_training_data = 0.5
            )
            training_data <- split_data$training_data
            validation_data <- split_data$testing_data
            summary_of_performance <- summarize_performance(
                type_of_model = "Logistic Regression",
                formula = default ~ income + balance,
                training_data = training_data,
                test_data = validation_data
            )
            error_rate <- summary_of_performance$error_rate
            print(error_rate)
            vector_of_error_rates <- append(vector_of_error_rates, error_rate)
        }
        mean(vector_of_error_rates)
        sd(vector_of_error_rates)
        ```
        
        Based on three validation approaches, the mean validation error rate is $2.5$ percent with a standard deviation of $0.1$ percent.
        
    (d) Now consider a logistic regression model that predicts the probability of `default` using `income`, `balance`, and a dummy variable for `student`. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for student leads to a reduction in the test error rate. 
        
        ```{r}
        number_of_observations <- nrow(Default)
        vector_of_indicators_of_whether_borrower_is_a_student <- rep(
            x = 0,
            times = number_of_observations
        )
        condition <- Default$student == "Yes"
        vector_of_indicators_of_whether_borrower_is_a_student[condition] <- 1
        data_frame <- data.frame(
            Default,
            indicator_of_whether_borrower_is_a_student =
                vector_of_indicators_of_whether_borrower_is_a_student
        )
        vector_of_error_rates <- double(length = 0)
        for (i in 2:4) {
            set.seed(i)
            split_data <- split_data_set_into_training_and_testing_data(
                data_frame = data_frame,
                proportion_of_training_data = 0.5
            )
            training_data <- split_data$training_data
            validation_data <- split_data$testing_data
            summary_of_performance <- summarize_performance(
                type_of_model = "Logistic Regression",
                formula = default
                    ~ income + balance + indicator_of_whether_borrower_is_a_student,
                training_data = training_data,
                test_data = validation_data
            )
            error_rate <- summary_of_performance$error_rate
            print(error_rate)
            vector_of_error_rates <- append(vector_of_error_rates, error_rate)
        }
        mean(vector_of_error_rates)
        sd(vector_of_error_rates)
        ```
        
        Including a dummy variable for student does not lead to a reduction in mean validation error rate; the mean validation error rate is higher by $0.1$ percent and its standard deviation is slightly lower.

6.  We continue to consider the use of a logistic regression model to predict the probability of `default` using `income` and `balance` on the `Default` data set.

    In particular, we will now compute estimates for the standard errors of the `income` and `balance` logistic regression coefficients in two different ways: (1) using the bootstrap, and (2) using the standard formula for computing the standard errors in the `glm()` function. Do not forget to set a random seed before beginning your analysis.

    (a) Using the `summary()` and `glm()` functions, determine the estimated standard errors for the coefficients associated with `income` and `balance` in a multiple logistic regression model that uses both predictors.
    
        ```{r}
        set.seed(1)
        LR_model <- glm(
            formula = default ~ income + balance,
            data = Default,
            family = binomial
        )
        summary(LR_model)
        ```
        
        The estimated standard errors for the coefficients associated with `income` and `balance` in a multiple logistic regression model with formula $default \sim income + balance$ are $4.985 \times 10^{-6}$ and $2.274 \times 10^{-4}$, respectively.

    (b) Write a function, `boot.fn()`, that takes as input the `Default` data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model.
    
        ```{r}
        boot.fn <- function(Default, vector_of_indices_of_training_observations) {
            LR_model <- glm(
                formula = default ~ income + balance,
                data = Default,
                family = "binomial",
                subset = vector_of_indices_of_training_observations
            )
            vector_of_coefficients <- coef(LR_model)
            return(vector_of_coefficients)
        }
        set.seed(1)
        vector_of_random_indices <- sample(1:number_of_observations)
        boot.fn(
            Default = Default,
            vector_of_indices_of_training_observations = vector_of_random_indices
        )
        ```

    (c) Use the `boot()` function together with your `boot.fn()` function to estimate the standard errors of the logistic regression coefficients for `income` and `balance`.
    
        ```{r}
        library(boot)
        set.seed(1)
        # statistic is a function which when applied to data
        # returns a vector containing statistic(s) of interest.
        # R is the number of bootstrap replicates.
        boot(data = Default, statistic = boot.fn, R = 10)
        ```
        
        Bootstrap estimates of the standard errors of the logistic regression coefficients for `income` and `balance` are $3.324 \times 10^{-6}$ and $1.752 \times 10^{-4}$.

    (d) Comment on the estimated standard errors obtained using the `glm()` function and using your bootstrap function.
    
        The rate of difference between the bootstrap estimates of the standard errors of the coefficients of the logistic regression model with formula $default \sim income + balance$ and the single-model estimates are $\left[\left(3.324 \times 10^{-6}\right) - \left(4.985 \times 10^{-6}\right)\right] / \left(4.985 \times 10^{-6}\right) = -0.333$ and $\left[\left(1.752 \times 10^{-4}\right) - \left(2.274 \times 10^{-4}\right)\right] / \left(2.274 \times 10^{-4}\right) = -0.230$.

7.  In Sections 5.3.2 and 5.3.3, we saw that the `cv.glm()` function can be used in order to compute the LOOCV test error estimate.

    Alternatively, one could compute those quantities using just the `glm()` and `predict.glm()` functions, and a for loop. You will now take this approach in order to compute the LOOCV error for a simple logistic regression model on the Weekly data set. Recall that in the context of classification problems, the LOOCV error is given in (5.4).

    (a) Fit a logistic regression model that predicts `Direction` using `Lag1` and `Lag2`.
    
        ```{r}
        set.seed(1)
        LR_model <- glm(
            formula = Direction ~ Lag1 + Lag2,
            data = Weekly,
            family = binomial
        )
        LR_model
        ```

    (b) Fit a logistic regression model that predicts `Direction` using `Lag1` and `Lag2` using all but the first observation.
    
        ```{r}
        set.seed(1)
        training_data <- Weekly[-1, ]
        testing_data <- Weekly[1, ]
        LR_model <- glm(
            formula = Direction ~ Lag1 + Lag2,
            data = training_data,
            family = binomial
        )
        LR_model
        ```

    (c) Use the model from (b) to predict the direction of the first observation. You can do this by predicting that the first observation will go up if $P$(Direction="Up"|Lag1, Lag2$) > 0.5$. Was this observation correctly classified?
    
        ```{r}
        number_of_test_observations <- nrow(testing_data)
        vector_of_predicted_probabilities <- predict(
            object = LR_model,
            newdata = testing_data,
            type = "response"
        )
        vector_of_predicted_directions <- rep("Down", number_of_test_observations)
        condition <- vector_of_predicted_probabilities > 0.5
        vector_of_predicted_directions[condition] = "Up"
        vector_of_predicted_directions
        testing_data
        ```
        
        The first observation was incorrectly classified.

    (d) Write a `for` loop from $i=1$ to $i=n$, where $n$ is the number of observations in the data set, that performs each of the following steps:

        i.  Fit a logistic regression model using all but the $i$-th observation to predict `Direction` using `Lag1` and `Lag2`.
        
        ii. Compute the posterior probability of the market moving up for the $i$-th observation.

        iii. Use the posterior probability for the $i$-th observation in order to predict whether or not the market moves up.

        iv. Determine whether or not an error was made in predicting the direction for the $i$-th observation. If an error was made, then indicate this as a 1, and otherwise indicate it as a 0.
        
            ```{r}
            number_of_observations <- nrow(Weekly)
            formula <- Direction ~ Lag1 + Lag2
            vector_of_indicators_of_error <- integer(0)
            for (i in 1:number_of_observations) {
                training_data <- Weekly[-i, ]
                testing_data <- Weekly[i, ]
                LR_model <- glm(
                    formula = formula,
                    data = training_data,
                    family = binomial
                )
                vector_of_predicted_probabilities <- predict(
                    object = LR_model,
                    newdata = testing_data,
                    type = "response"
                )
                vector_of_predicted_directions <- rep("Down", number_of_test_observations)
                condition <- vector_of_predicted_probabilities > 0.5
                vector_of_predicted_directions[condition] = "Up"
                if (vector_of_predicted_directions != testing_data$Direction) {
                    vector_of_indicators_of_error <-
                        append(vector_of_indicators_of_error, 1)
                } else {
                    vector_of_indicators_of_error <-
                        append(vector_of_indicators_of_error, 0)
                }
            }
            ```

    (e) Take the average of the n numbers obtained in (d) in order to obtain the LOOCV estimate for the test error. Comment on the results.
    
        ```{r}
        mean(vector_of_indicators_of_error)
        ```

8.  We will now perform cross-validation on a simulated data set.

    (a) Generate a simulated data set as follows:
    
        ```{r}
        set.seed(1)
        y <- rnorm(100)
        x <- rnorm(100)
        y <- x - 2*x^2 + rnorm(100)
        ```

        In this data set, what is $n$ and what is $p$? Write out the model used to generate the data in equation form.
        
        The number of observations $n = 100$. The number of predictors $p = 2$. Here, a predictor is considered to be a predictive term as opposed single variable $x$. The model used to generate the data
        
        $$\boldsymbol{y} = rnorm(100) + \boldsymbol{x} - 2 \boldsymbol{x}^2$$
        $$y = rnorm(1) + x - 2 x^2$$

    (b) Create a scatterplot of `x` against `y`. Comment on what you find.
    
        ```{r}
        plot(x = x, y = y)
        ```
        
        This scatterplot suggests that $y$ relates to $x$ quadratically.

    (c) Set a random seed, and then compute the LOOCV errors that result from fitting the following four models using least squares:

        i.  $Y = \beta_0 + \beta_1X + \epsilon$
        
            ```{r}
            set.seed(1)
            data_frame <- data.frame(x = x, y = y)
            glm_1 <- glm(formula = y ~ x)
            boot::cv.glm(data = data_frame, glmfit = glm_1)$delta[1]
            ```
        
        ii. $Y = \beta_0 + \beta_1X + \beta_2X^2 + \epsilon$
        
            ```{r}
            glm_2 <- glm(formula = y ~ poly(x, 2))
            boot::cv.glm(data = data_frame, glmfit = glm_2)$delta[1]
            ```
        
        iii. $Y = \beta_0 +\beta_1X +\beta_2X^2 +\beta_3X^3 +\epsilon$
        
             ```{r}
             glm_3 <- glm(formula = y ~ poly(x, 3))
             boot::cv.glm(data = data_frame, glmfit = glm_3)$delta[1]
             ```
        
        iv. $Y = \beta_0 +\beta_1X +\beta_2X^2 +\beta_3X^3 +\beta_4X^4 +\epsilon$.
        
            ```{r}
            glm_4 <- glm(formula = y ~ poly(x, 4))
            boot::cv.glm(data = data_frame, glmfit = glm_4)$delta[1]
            ```

        Note you may find it helpful to use the `data.frame()` function to create a single data set containing both `X` and `Y`.

    (d) Repeat (c) using another random seed, and report your results. Are your results the same as what you got in (c)? Why?
    
        ```{r}
        set.seed(2)
        glm_1 <- glm(formula = y ~ x)
        boot::cv.glm(data = data_frame, glmfit = the_glm)$delta[1]
        glm_2 <- glm(formula = y ~ poly(x, 2))
        boot::cv.glm(data = data_frame, glmfit = glm_2)$delta[1]
        glm_3 <- glm(formula = y ~ poly(x, 3))
        boot::cv.glm(data = data_frame, glmfit = glm_3)$delta[1]
        glm_4 <- glm(formula = y ~ poly(x, 4))
        boot::cv.glm(data = data_frame, glmfit = glm_4)$delta[1]
        ```
        
        Given that each LOOCV error rate that is averaged is based on the same training and testing data sets regardless of seed, our results are the same as what we got in (c).

    (e) Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer.
    
        The quartic model in (c) has the smallest LOOCV error. The average mean squared error for the quartic model is slightly less than the average mean squared error for the quadratic model. We expected the average mean squared error for the quadratic model to be less than the average mean squared error for the other models, though it seems reasonable that polynomial of degree greater than $2$ would fit noisy data from a quadratic function better than the quadratic function.

    (f) Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?
    
        ```{r}
        summary(glm_1)$coefficients
        summary(glm_2)$coefficients
        summary(glm_3)$coefficients
        summary(glm_4)$coefficients
        ```
        
        $x$ is significant in the context of the first-degree polynomial model.
        $x$ and $x^2$ are significant in the context of the second-degree polynomial model.
        $x$ and $x^2$ but not $x^3$ are significant in the context of the third-degree polynomial model.
        $x$ and $x^2$ but not $x^3$ and $x^4$ are significant in the context of the four-degree polynomial model.
        
        These results seem to suggest that a quadratic model might be the best fit. Given that the average mean squared errors for our nonlinear models are similar, these results agree with the conclusions drawn based on the cross-validation results.