---
title: "DS-6030 Homework Module 5"
author: "Tom Lever"
date: 06/24/2023
output:
  pdf_document: default
  html_document: default
urlcolor: blue
---

<!--- Below are global settings for knitr. You can override any of them by adding the changes to individual chunks --->
```{r global_options, include = FALSE}
# This chunk is called global_options. Due to `include = FALSE`, when the document is rendered, the chunk will be executed, but the results and code will not be included in the rendered document
knitr::opts_chunk$set(
    error = TRUE, # Keep compiling upon error
    collapse = FALSE, # code and corresponding output appear in knit file in separate blocks
    echo = TRUE, # echo code by default
    comment = "#", # change comment character
    #fig.width = 5.5, # set figure width
    fig.align = "center", # set figure position
    #out.width = "49%", # set width of displayed images
    warning = TRUE, # do not show R warnings
    message = TRUE # do not show R messages
)
```

**DS 6030 | Spring 2023 | University of Virginia **

8.  In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.

    (a) Use the `rnorm()` function to generate a predictor $X$ of length $n = 100$, as well as a noise vector $\epsilon$ of length $n = 100$.
    
        ```{r}
        set.seed(2)
        X <- rnorm(n = 100, mean = 0, sd = 1)
        epsilon <- rnorm(n = 100, mean = 0, sd = 1*10^{-6})
        X[1:3]
        epsilon[1:3]
        ```

    (b) Generate a response vector $Y$ of length $n = 100$ according to the model $Y = \beta_0 + \beta_1X + \beta_2X^2 + \beta_3X^3 + \epsilon$, where $\beta_0$, $\beta_1$, $\beta_2$, $\beta_3$  are constants of your choice.
    
        ```{r}
        beta_0 <- 1
        beta_1 <- 2
        beta_2 <- 3
        beta_3 <- 4
        Y <- beta_0 + beta_1 * X + beta_2 * I(X^2) + beta_3 * I(X^3) + epsilon
        Y[1:3]
        ```

    (c) Use the `regsubsets()` function to perform best subset selection in order to choose the best model containing the predictors $X, X^2 \dots, X^{10}$. What is the best model obtained according to $Cp$, BIC, and adjusted $R^2$? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the `data.frame()` function to create a single data set containing both $X$ and $Y$.
    
        ```{r, eval=TRUE, echo=TRUE, results="show", message=FALSE}
        data_frame <- data.frame(X = X, Y = Y)
        head(data_frame, n = 3)
        library(leaps)
        formula <- Y ~ X + I(X^2) + I(X^3) + I(X^4) + I(X^5) + I(X^6) + I(X^7) + I(X^8) + I(X^9) + I(X^10)
        subset_selection_object <- regsubsets(
            x = formula,
            data = data_frame,
            method = "exhaustive"
        )
        library(TomLeversRPackage)
        analyze_subset_selection_object(subset_selection_object)
        ```
        
        The best model obtained according to Mallow's $C_p$, the Schwartz Bayesian Information Criterion, and adjusted $R^2$ is
        $$Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 = 1 + 2 X + 3 X^2 + 4 X^3$$

    (d) Repeat (c), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (c)?
    
        The models chosen by forward and backward selection involve the true predictive terms and their coefficients.
    
        ```{r}
        subset_selection_object <- regsubsets(
            x = formula,
            data = data_frame,
            method = "forward"
        )
        analyze_subset_selection_object(subset_selection_object)
        subset_selection_object <- regsubsets(
            x = formula,
            data = data_frame,
            method = "backward"
        )
        analyze_subset_selection_object(subset_selection_object)
        ```

        The best model obtained according to Mallow's $C_p$, the Schwartz Bayesian Information Criterion, and adjusted $R^2$ is
        $$Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 = 1 + 2 X + 3 X^2 + 4 X^3$$
        
        These results are identical to those in part (c).

    (e) Now fit a lasso model to the simulated data, again using $X, X^2 \dots, X^{10}$ as predictors. Use cross-validation to select the optimal value of $\lambda$. Create plots of the cross-validation error as a function of $\lambda$. Report the resulting coefficient estimates, and discuss the results obtained.
    
        ```{r}
        full_model_matrix <- model.matrix(object = formula, data = data_frame)[, -1]
        the_cv.glmnet <- glmnet::cv.glmnet(x = full_model_matrix, y = Y, alpha = 1)
        plot(the_cv.glmnet)
        optimal_value_of_lambda <- the_cv.glmnet$lambda.min
        optimal_value_of_lambda
        polynomial_lasso_regression_model <- glmnet::glmnet(full_model_matrix, y = Y, alpha = 1)
        predict(object = polynomial_lasso_regression_model, s = optimal_value_of_lambda, type = "coefficients")
        ```
        
        The models chosen by polynomial lasso regression involve the true predictive terms and rough approximations of their coefficients. $\beta_0 = 1.364$, $\beta_1 = 1.991$, $\beta_2 = 2.730$, and $\beta_3 = 3.899$.

    (f) Now generate a response vector $Y$ according to the model $Y = \beta_0 + \beta_7X^7 + \epsilon$, and perform best subset selection and the lasso. Discuss the results obtained.
    
        ```{r}
        beta_7 <- 8
        Y <- beta_0 + beta_7 * I(X^7) + epsilon
        data_frame <- data.frame(X = X, Y = Y)
        head(data_frame, n = 3)
        #library(leaps)
        formula <- Y ~ X + I(X^2) + I(X^3) + I(X^4) + I(X^5) + I(X^6) + I(X^7) + I(X^8) + I(X^9) + I(X^10)
        subset_selection_object <- regsubsets(
            x = formula,
            data = data_frame,
            method = "exhaustive"
        )
        analyze_subset_selection_object(subset_selection_object)
        ```
    
        The best model obtained according to Mallow's $C_p$, the Schwartz Bayesian Information Criterion, and adjusted $R^2$ is
        $$Y = \beta_0 + \beta_7 X^7 = 1 + 8 X^7$$
        
        ```{r}
        full_model_matrix <- model.matrix(object = formula, data = data_frame)[, -1]
        the_cv.glmnet <- glmnet::cv.glmnet(x = full_model_matrix, y = Y, alpha = 1)
        plot(the_cv.glmnet)
        optimal_value_of_lambda <- the_cv.glmnet$lambda.min
        optimal_value_of_lambda
        polynomial_lasso_regression_model <- glmnet::glmnet(full_model_matrix, y = Y, alpha = 1)
        predict(object = polynomial_lasso_regression_model, s = optimal_value_of_lambda, type = "coefficients")
        ```
        
        The models chosen by polynomial lasso regression involve the true predictive terms and rough approximations of their coefficients. $\beta_0 = 1.364$ and $\beta_7 = 7.767$.
    
# 9. In this exercise, we will predict the number of applications received using the other variables in the College data set.

(a) Split the data set into a training set and a test set.

    ```{r}
    colleges <- ISLR2::College
    head(colleges, n = 3)
    proportion_of_training_data <- 0.9
    library(TomLeversRPackage)
    split_data <- split_data_set_into_training_and_testing_data(
        data_frame = colleges,
        proportion_of_training_data = proportion_of_training_data
    )
    vector_of_indices_of_training_data <- split_data$vector_of_indices_of_training_data
    vector_of_indices_of_training_data[1:3]
    vector_of_indices_of_testing_data <- split_data$vector_of_indices_of_testing_data
    vector_of_indices_of_testing_data[1:3]
    training_data <- split_data$training_data
    head(training_data, n = 3)
    testing_data <- split_data$testing_data
    head(testing_data, n = 3)
    ```

(b) Fit a linear model using least squares on the training set, and report the test error obtained.

    ```{r}
    formula = Apps ~ .
    linear_model <- lm(formula, data = training_data)
    calculate_mean_squared_error(linear_model)
    y_hat_linear <- predict(
        object = linear_model,
        testing_data
    )
    testing_vector_of_numbers_of_applications <-
        colleges$Apps[vector_of_indices_of_testing_data]
    y <- testing_vector_of_numbers_of_applications
    y_bar <- mean(testing_vector_of_numbers_of_applications)
    R_squared <- 1 - mean((y_hat_linear - y)^2) / mean((y_bar - y)^2)
    R_squared
    ```
    
    The resulting mean squared error of prediction $MSEP_{linear} = 1,043,968$.

(c) Fit a ridge regression model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained.

    ```{r}
    full_model_matrix <- model.matrix(object = formula, data = colleges)[, -1]
    the_cv.glmnet <- glmnet::cv.glmnet(
        x = full_model_matrix,
        y = colleges$Apps,
        alpha = 0
    )
    plot(the_cv.glmnet)
    optimal_value_of_lambda <- the_cv.glmnet$lambda.min
    optimal_value_of_lambda
    training_data_frame <- colleges[vector_of_indices_of_training_data, ]
    training_model_matrix <- model.matrix(
        object = formula,
        data = training_data_frame
    )[, -1]
    training_vector_of_numbers_of_applications <-
        colleges$Apps[vector_of_indices_of_training_data]
    polynomial_ridge_regression_model <- glmnet::glmnet(
        x = training_model_matrix,
        y = training_vector_of_numbers_of_applications,
        alpha = 0
    )
    testing_data_frame <- colleges[vector_of_indices_of_testing_data, ]
    testing_model_matrix <- model.matrix(
        object = formula,
        data = testing_data_frame
    )[, -1]
    vector_of_predicted_numbers_of_applications <- predict(
        object = polynomial_ridge_regression_model,
        s = optimal_value_of_lambda,
        newx = testing_model_matrix
    )[, 1]
    vector_of_residuals <-
        vector_of_predicted_numbers_of_applications -
        testing_vector_of_numbers_of_applications
    vector_of_squared_residuals <- vector_of_residuals^2
    mean_squared_error <- mean(vector_of_squared_residuals)
    mean_squared_error
    y_hat_ridge <- vector_of_predicted_numbers_of_applications
    R_squared <- 1 - mean((y_hat_ridge - y)^2) / mean((y_bar - y)^2)
    R_squared
    ```
    
    The resulting mean squared error of prediction $MSEP_{ridge} = 1,071,002$. $MSEP_{linear} < MSEP_{ridge}$.

(d) Fit a lasso model on the training set, with $\lambda$ chosen by cross-validation. Report the test error obtained, along with the number of non-zero coefficient estimates.

    ```{r}
    full_model_matrix <- model.matrix(object = formula, data = colleges)[, -1]
    the_cv.glmnet <- glmnet::cv.glmnet(
        x = full_model_matrix,
        y = colleges$Apps,
        alpha = 1
    )
    plot(the_cv.glmnet)
    optimal_value_of_lambda <- the_cv.glmnet$lambda.min
    optimal_value_of_lambda
    
    training_model_matrix <- model.matrix(
        object = formula,
        data = training_data_frame
    )[, -1]
    training_vector_of_numbers_of_applications <-
        colleges$Apps[vector_of_indices_of_training_data]
    testing_vector_of_numbers_of_applications <-
        colleges$Apps[vector_of_indices_of_testing_data]
    polynomial_lasso_regression_model <- glmnet::glmnet(
        x = training_model_matrix,
        y = training_vector_of_numbers_of_applications,
        alpha = 1
    )
    testing_model_matrix <- model.matrix(
        object = formula,
        data = testing_data_frame
    )[, -1]
    vector_of_predicted_numbers_of_applications <- predict(
        object = polynomial_lasso_regression_model,
        s = optimal_value_of_lambda,
        newx = testing_model_matrix
    )[, 1]
    vector_of_residuals <-
        vector_of_predicted_numbers_of_applications -
        testing_vector_of_numbers_of_applications
    vector_of_squared_residuals <- vector_of_residuals^2
    mean_squared_error <- mean(vector_of_squared_residuals)
    mean_squared_error
    predict(
        object = polynomial_lasso_regression_model,
        s = optimal_value_of_lambda,
        type = "coefficients"
    )
    y_hat_lasso <- vector_of_predicted_numbers_of_applications
    R_squared <- 1 - mean((y_hat_lasso - y)^2) / mean((y_bar - y)^2)
    R_squared
    ```
    
    The number of non-zero coefficients $M = 17$, which is equal to the number of non-zero coefficients. The resulting mean squared error of prediction $MSEP_{lasso} = 1,168,351$. $MSEP_{linear} < MSEP_{ridge} < MSEP_{lasso}$.

(e) Fit a PCR model on the training set, with $M$ chosen by cross-validation. Report the test error obtained, along with the value of $M$ selected by cross-validation.

    ```{r}
    library(pls)
    the_mvr <- pcr(formula, data = training_data_frame, scale = TRUE, validation = "CV")
    validationplot(the_mvr, val.type = "MSEP")
    vector_of_predicted_numbers_of_applications <- predict(
        object = the_mvr,
        testing_data_frame,
        ncomp = 17
    )
    vector_of_residuals <-
        vector_of_predicted_numbers_of_applications -
        testing_vector_of_numbers_of_applications
    vector_of_squared_residuals <- vector_of_residuals^2
    mean_squared_error <- mean(vector_of_squared_residuals)
    mean_squared_error
    y_hat_PCR <- vector_of_predicted_numbers_of_applications
    R_squared <- 1 - mean((y_hat_PCR - y)^2) / mean((y_bar - y)^2)
    R_squared
    ```
    
    The resulting mean squared error of prediction $MSEP_{PCR} = 1,231,856$. $MSEP_{linear} < MSEP_{ridge} < MSEP_{lasso} < MSEP_{PCR}$.

(f) Fit a PLS model on the training set, with $M$ chosen by cross-validation. Report the test error obtained, along with the value of $M$ selected by cross-validation.

    ```{r}
    library(pls)
    the_mvr <- plsr(formula, data = training_data_frame, scale = TRUE, validation = "CV")
    validationplot(the_mvr, val.type = "MSEP")
    vector_of_predicted_numbers_of_applications <- predict(
        object = the_mvr,
        testing_data_frame,
        ncomp = 17
    )
    vector_of_residuals <-
        vector_of_predicted_numbers_of_applications -
        testing_vector_of_numbers_of_applications
    vector_of_squared_residuals <- vector_of_residuals^2
    mean_squared_error <- mean(vector_of_squared_residuals)
    mean_squared_error
    y_hat_PLSR <- vector_of_predicted_numbers_of_applications
    R_squared <- 1 - mean((y_hat_PLSR - y)^2) / mean((y_bar - y)^2)
    R_squared
    ```

    The number of non-zero coefficients estimates is $17$. The resulting mean squared error of prediction $MSEP_{PLSR} = 1,231,856$. $MSEP_{linear} < MSEP_{ridge} < MSEP_{lasso} < MSEP_{PCR}; MSEP_{PCR} = MSEP_{PLSR}$.

(g) Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches?

    As above, $MSEP_{linear} < MSEP_{ridge} < MSEP_{lasso} < MSEP_{PCR}; MSEP_{PCR} = MSEP_{PLSR}$. The mean squared error of prediction of the linear model $MSEP_{linear} = 1,043,968$. The mean squared error of prediction of the PLSR model $MSEP_{PLSR} = 1,231,856$. The rate of difference between these errors is $-0.153$. There is a strong effect for each model; ${R^2}_{linear} = {R^2}_{PCR} = {R^2}_{PLSR}; {R^2}_{linear} < {R^2}_{lasso} < {R^2}_{ridge}$.