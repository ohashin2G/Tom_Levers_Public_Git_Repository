---
title: "Building A Model That Helps Locating Displaced People"
author: "Tom Lever"
date: 06/08/2023
output:
  pdf_document: default
  html_document: default
urlcolor: blue
---

<!--- Below are global settings for knitr. You can override any of them by adding the changes to individual chunks --->
```{r global_options, include = FALSE}
# This chunk is called global_options. Due to `include = FALSE`, when the document is rendered, the chunk will be executed, but the results and code will not be included in the rendered document
knitr::opts_chunk$set(
    error = TRUE, # Keep compiling upon error
    collapse = FALSE, # code and corresponding output appear in knit file in separate blocks
    echo = TRUE, # echo code by default
    comment = "#", # change comment character
    #fig.width = 5.5, # set figure width
    fig.align = "center", # set figure position
    #out.width = "49%", # set width of displayed images
    warning = TRUE, # do not show R warnings
    message = TRUE # do not show R messages
)
```

In this project, we build a model that would help us locate people displaced by the earthquake in Haiti in $2010$. More specifically, we build in a timely manner an accurate model that classifies pixels in geo-referenced aerial images of Haiti in $2010$ as depicting blue tarps or depicting objects that are not blue tarps. People whose homes were destroyed by the earthquake often created temporary shelters using blue tarps. Blue tarps were good indicators of where displaced people lived.

Our training data was collected likely by applying a Region Of Interest (ROI) Tool to a high-resolution, orthorectified / geo-referenced image of Haiti in 2010. One ROI tool is described at [https://www.l3harrisgeospatial.com/docs/regionofinteresttool.html](https://www.l3harrisgeospatial.com/docs/regionofinteresttool.html). Classes may be assigned to pixels by defining Regions Of Interest.

Our training data frame consists of $63,241$ observations. Each observation consists of a class in the set $\{Vegetation, \ Soil, \ Rooftop, \ Various \ Non-Tarp, \ Blue \ Tarp\}$ and a pixel. A pixel is a colored dot. A pixel is represented by a tuple of intensities of color $Red$, $Green$, and $Blue$ in the range $0$ to $255$.

According to [https://www.esri.com/about/newsroom/insider/what-is-orthorectified-imagery/](https://www.esri.com/about/newsroom/insider/what-is-orthorectified-imagery/), an orthorectified image is an accurately georeferenced image that has been processed so that all pixels are in an accurate $(x, y)$ position on the ground. Orthorectified images have been processed to apply corrections for optical distortions from the sensor system, and apparent changes in the position of ground objects caused by the perspective of the sensor view angle and ground terrain.

We load a data frame of classes and pixels based on an orthorectified image of Haiti at [https://www.kaggle.com/datasets/billbasener/pixel-values-from-images-over-haiti?datasetId=1899167](https://www.kaggle.com/datasets/billbasener/pixel-values-from-images-over-haiti?datasetId=1899167).



```{r, echo = FALSE}
data_frame_of_classes_and_pixels <- read.csv(
    file = "Data_Frame_Of_Classes_And_Pixels.csv"
)
head(x = data_frame_of_classes_and_pixels, n = 2)
tail(x = data_frame_of_classes_and_pixels, n = 2)
```

We build binary classifiers that classify pixels as depicting blue tarps or depicting objects that are not blue tarps. We may ignore non-binary classifiers that predict probabilities for all classes and may be used to locate pixels that more likely depict blue tarps than objects that are not blue tarps. We may ignore non-binary classifiers since the intensity space for pixels representing blue tarps is distinct from the intensity space for pixels representing objects that are not blue tarps. See "Figure 1: Distribution Of Classes In Intensity Space".

```{r, fig.cap = "Distribution Of Classes In Intensity Space", echo = FALSE, eval = FALSE}
library(rgl)
vector_of_possible_colors <- c("green", "brown", "black", "orange", "blue")
number_of_observations <- nrow(data_frame_of_classes_and_pixels)
column_of_numerical_representations <- rep(0, number_of_observations)
condition <- data_frame_of_classes_and_pixels$Class == "Vegetation"
column_of_numerical_representations[condition] <- 1
condition <- data_frame_of_classes_and_pixels$Class == "Soil"
column_of_numerical_representations[condition] <- 2
condition <- data_frame_of_classes_and_pixels$Class == "Rooftop"
column_of_numerical_representations[condition] <- 3
condition <- data_frame_of_classes_and_pixels$Class == "Various Non-Tarp"
column_of_numerical_representations[condition] <- 4
condition <- data_frame_of_classes_and_pixels$Class == "Blue Tarp"
column_of_numerical_representations[condition] <- 5
vector_of_colors <- vector_of_possible_colors[column_of_numerical_representations]
plot3d(
    x = data_frame_of_classes_and_pixels$Red,
    y = data_frame_of_classes_and_pixels$Green,
    z = data_frame_of_classes_and_pixels$Blue,
    col = vector_of_colors,
    xlab = "Red",
    ylab = "Green",
    zlab = "Blue"
)
rglwidget()
```

In order to build binary classifiers, we create a data frame with a column of indicators of whether of not a pixel depicts a blue tarp instead of a column of classes. We normalize as opposed to standardize intensities given that distributions of intensity are not normal.

```{r, fig.cap = "Distribution Of Classes In Intensity Space", echo = FALSE}
library(rgl)
number_of_observations <- nrow(data_frame_of_classes_and_pixels)
column_of_indicators <- rep(0, number_of_observations)
condition <- data_frame_of_classes_and_pixels$Class == "Blue Tarp"
column_of_indicators[condition] <- 1
factor_of_indicators <- factor(column_of_indicators)
data_frame_of_indicators_and_pixels <- data.frame(
    Indicator = factor_of_indicators,
    Red = data_frame_of_classes_and_pixels$Red,
    Green = data_frame_of_classes_and_pixels$Green,
    Blue = data_frame_of_classes_and_pixels$Blue
)
vector_of_random_indices <- sample(1:number_of_observations)
data_frame_of_indicators_and_pixels <- data_frame_of_indicators_and_pixels[vector_of_random_indices, ]
#mean_Red_intensity <- mean(data_frame_of_indicators_and_pixels$Red)
#standard_deviation_of_Red_intensity <- sd(data_frame_of_indicators_and_pixels$Red)
#mean_Green_intensity <- mean(data_frame_of_indicators_and_pixels$Green)
#standard_deviation_of_Green_intensity <- sd(data_frame_of_indicators_and_pixels$Green)
#mean_Blue_intensity <- mean(data_frame_of_indicators_and_pixels$Blue)
#standard_deviation_of_Blue_intensity <- sd(data_frame_of_indicators_and_pixels$Blue)
#data_frame_of_indicators_and_pixels$Red <- (data_frame_of_indicators_and_pixels$Red - mean_Red_intensity) / standard_deviation_of_Red_intensity
#data_frame_of_indicators_and_pixels$Green <- (data_frame_of_indicators_and_pixels$Green - mean_Green_intensity) / standard_deviation_of_Green_intensity
#data_frame_of_indicators_and_pixels$Blue <- (data_frame_of_indicators_and_pixels$Blue - mean_Blue_intensity) / standard_deviation_of_Blue_intensity
data_frame_of_indicators_and_pixels$Red <- data_frame_of_indicators_and_pixels$Red / 255
data_frame_of_indicators_and_pixels$Green <- data_frame_of_indicators_and_pixels$Green / 255
data_frame_of_indicators_and_pixels$Blue <- data_frame_of_indicators_and_pixels$Blue / 255
head(x = data_frame_of_indicators_and_pixels, n = 2)
tail(x = data_frame_of_indicators_and_pixels, n = 2)
```

We use $10$-fold cross-validation to evaluate the performance of $5$ classifiers. A classifier will classify a pixel as depicting a blue tarp or depicting an object that is not a blue tarp. We consider graphs each with an increasing purple curve representing Average Precision vs. Threshold, a decreasing red curve representing Average Recall vs. Threshold, a forest-green curve representing Average F1 Measure vs. Threshold, and a teal curve representing Average Decimal Of True Positives vs. Threshold.

A threshold is a probability between $0$ and $1$. A model classifies a pixel as representing a blue tarp if the model the probability that the pixel represents a blue tarp that the model predicts is greater than the threshold. Precision is the ratio of true positives to predicted positives. Recall is the ratio of true positives to actual positives. An F1 measure is the harmonic mean of precision and recall. A decimal of true positives is the ratio of number of true positives to total number of predictions.

When tuning the thresholds of our models we prioritize recall at least as much as precision. We prioritize identifying as many positives correctly as possible over having predicted positives be correct. We recommend models with thresholds less than or equal to the threshold $t$ that corresponds to the maximum F1 measure and that is least. Note that precision will decrease more rapidly than recall will increase for thresholds less than $t$. Our ideal threshold may be $t$ or may be the highest threshold at which the derivative of Average Precision vs. Threshold is $1$.

We consider how the number of refugees to visit may change with precision. We assume that the testing data consists of actual indicators and random pixels from the training image. Suppose precision $PPV = TP / \hat{P} = 0.944$. Suppose decimal of true positives $TPD = TP / n = 0.0296$. The decimal of predicted positives $PPD = \hat{P} / n = TPD / PPV = \frac{TP/n}{TP/\hat{P}} = 0.0296 / 0.944 = 0.0313$. The number of pixels in our training image is $4441 \times 6833 = 30,345,353$. The number of pixels in our training image that might be predicted to represent blue tarps is $949,810$. Considering a blue tarp in the top left corner of our training image, the number of pixels representing one blue tarp may be $36 \times 36 = 1,296$. The predicted number of blue tarps in our training image may be $733$. We assume that there is one refugee per blue tarp. If we change precision by a factor of -0.139 to 0.8125, the predicted number of blue tarps in our training image may increase by $120$ to $853$. The number of true positives may increase by $4$ and the number of false negatives may decrease by $4$.

Suppose the width of our training image is $1,500 \ ft$. Then our training image's height is about $2,308 \ ft$ and its area is about $0.124 \ mi^2$. Suppose the area of Léogâne is $148.7 \ mi^2. There are about $1,200$ areas like that depicted in our training image in Léogâne.

We use $10$-fold cross-validation to evaluate the performance of logistic-regression models. We perform manual bidirectional selection. According to [https://www.ibm.com/docs/en/contentclassificatio/8.8?topic=analysis-category-graph-precision-recall-vs-threshold](https://www.ibm.com/docs/en/contentclassificatio/8.8?topic=analysis-category-graph-precision-recall-vs-threshold), "The ideal threshold setting is the highest possible recall and precision rate. This goal is not always achievable, because the higher the recall rate, the lower the precision rate, and vice versa. Setting the most appropriate threshold for a category is a trade-off between these two rates." We consider the ideal threshold setting to correspond to the highest F1 measure. First, we consider the addition to an intercept only logistic regression model of one of the predictive terms $Red$, $Green$, $Blue$, $log(Red)$, $log(Green)$, $log(Blue)$, $sqrt(Red)$, $sqrt(Green)$, $sqrt(Blue)$, $Red^2$, $Green^2$, and $Blue^2$. The logistic regression model with formula $Indicator ~ Blue^2$ has the highest F1 measure. We consider the addition of each of the above predictive terms as well as $Red:Green$, $Red:Blue$, and $Green:Blue$. The logistic regression model with formula $Indicator ~ Blue^2 + Red:Green$ has the highest F1 measure. We do not remove predictive term $Blue^2$ from this model. We consider the addition of each of the above predictive terms. The logistic regression model with formula $Indicator ~ Blue^2 + Red:Green + Green$ has the highest F1 measure of $0.943$. We do not remove any predictive term because we have already considered submodels. We consider the addition of each of the above predictive terms. No model has a higher F1 measure than the logistic regression model with formula $Indicator ~ Blue^2 + Red:Green + Green$.

Following the same process, our best Linear Discriminant Analysis model has formula $Indicator ~ Blue^2 + Red:Blue + sqrt(Blue) + log(Blue) + Blue$ and F1 measure $0.906$.

Our best Quadratic Discriminant Analysis model has formula $Indicator ~ Blue^2 + Red^2 + Red:Green + Red + Blue + Green$ and F1 measure $0.952$.

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Precision / Recall Curve For Logistic Regression Model With Formula Indicator vs. Blue Squared + Interaction Of Red And Green + Green", eval = FALSE}
library(TomLeversRPackage)
#summarize_performance_of_cross_validated_models_using_dplyr(
#    type_of_model = "Logistic Regression",
#    formula = Indicator ~ I(Blue^2),
#    data_frame = data_frame_of_indicators_and_pixels
#)
#print("Indicator ~ I(Blue^2) + Red:Green")
#summarize_performance_of_cross_validated_models_using_dplyr(
#    type_of_model = "Logistic Regression",
#    formula = Indicator ~ I(Blue^2) + Red:Green,
#    data_frame = data_frame_of_indicators_and_pixels
#)
summarize_performance_of_cross_validated_models_using_dplyr(
    type_of_model = "Logistic Regression",
    formula = Indicator ~ I(Blue^2) + Red:Green + Green,
    data_frame = data_frame_of_indicators_and_pixels
)
```

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Precision / Recall Curve For Logistic Regression Model With Formula Indicator vs. TBD", eval = FALSE}
library(TomLeversRPackage)
summarize_performance_of_cross_validated_models_using_dplyr(
    type_of_model = "Logistic Ridge Regression",
    formula = Indicator ~ Red + Blue + Green,
    data_frame = data_frame_of_indicators_and_pixels
)
```

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Precision / Recall Curve For Linear Discriminant Analysis Model With Formula Indicator vs. Blue Squared + Interaction Of Red And Blue + Square Root Of Blue + Natural Logarithm Of Blue + Blue", eval = FALSE}
library(TomLeversRPackage)
#summarize_performance_of_cross_validated_models_using_dplyr(
#    type_of_model = "LDA",
#    formula = Indicator ~ I(Blue^2),
#    data_frame = data_frame_of_indicators_and_pixels
#)
#summarize_performance_of_cross_validated_models_using_dplyr(
#    type_of_model = "LDA",
#    formula = Indicator ~ I(Blue^2) + Red:Blue,
#    data_frame = data_frame_of_indicators_and_pixels
#)
#summarize_performance_of_cross_validated_models_using_dplyr(
#    type_of_model = "LDA",
#    formula = Indicator ~ I(Blue^2) + Red:Blue + sqrt(Blue),
#    data_frame = data_frame_of_indicators_and_pixels
#)
summarize_performance_of_cross_validated_models_using_dplyr(
    type_of_model = "LDA",
    formula = Indicator ~ I(Blue^2) + Red:Blue + sqrt(Blue) + log(Blue) + Blue,
    data_frame = data_frame_of_indicators_and_pixels
)
```

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Precision / Recall Curve For Quadratic Discriminant Analysis Model With Formula Indicator vs. Blue Squared + Red Squared + Interaction Of Red And Green + Red + Natural Logarithm Of Blue + Natural Logarithm Of Green", eval = FALSE}
library(TomLeversRPackage)
#summarize_performance_of_cross_validated_models_using_dplyr(
#    type_of_model = "QDA",
#    formula = Indicator ~ I(Blue^2),
#    data_frame = data_frame_of_indicators_and_pixels
#)
#summarize_performance_of_cross_validated_models_using_dplyr(
#    type_of_model = "QDA",
#    formula = Indicator ~ I(Blue^2) + I(Red^2),
#    data_frame = data_frame_of_indicators_and_pixels
#)
#summarize_performance_of_cross_validated_models_using_dplyr(
#    type_of_model = "QDA",
#    formula = Indicator ~ I(Blue^2) + I(Red^2) + Red:Green,
#    data_frame = data_frame_of_indicators_and_pixels
#)
#summarize_performance_of_cross_validated_models_using_dplyr(
#    type_of_model = "QDA",
#    formula = Indicator ~ I(Blue^2) + I(Red^2) + Red:Green + Red,
#    data_frame = data_frame_of_indicators_and_pixels
#)
#summarize_performance_of_cross_validated_models_using_dplyr(
#    type_of_model = "QDA",
#    formula = Indicator ~ I(Blue^2) + I(Red^2) + Red:Green + Red + log(Blue),
#    data_frame = data_frame_of_indicators_and_pixels
#)
summarize_performance_of_cross_validated_models_using_dplyr(
    type_of_model = "QDA",
    formula = Indicator ~ I(Blue^2) + I(Red^2) + Red:Green + Red + log(Blue) + log(Green),
    data_frame = data_frame_of_indicators_and_pixels
)
```

```{r, echo = FALSE, warning = FALSE, message = FALSE, fig.cap = "Precision / Recall Curve For K Nearest Neighbors Model With Formula Indicator vs. Red:Blue And Possibly More Predictive Terms", eval = TRUE}
library(TomLeversRPackage)
summarize_performance_of_cross_validated_models_using_dplyr(
    type_of_model = "KNN",
    formula = Indicator ~ Red:Blue,
    data_frame = data_frame_of_indicators_and_pixels
)
```